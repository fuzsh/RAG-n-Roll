\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Ablation Study}{57}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ablation}{{5}{57}{Ablation Study}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Evaluation Methodology}{58}{section.5.1}\protected@file@percent }
\newlabel{sec:evaluation-methodology}{{5.1}{58}{Evaluation Methodology}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Iterative Optimization Process}{58}{subsection.5.1.1}\protected@file@percent }
\newlabel{subsec:iterative-optimization-process}{{5.1.1}{58}{Iterative Optimization Process}{subsection.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Evaluation Metrics}{58}{subsection.5.1.2}\protected@file@percent }
\newlabel{subsec:ablation-metrics}{{5.1.2}{58}{Evaluation Metrics}{subsection.5.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Significance of the Methodology}{59}{subsection.5.1.3}\protected@file@percent }
\newlabel{subsec:significance-of-the-methodology}{{5.1.3}{59}{Significance of the Methodology}{subsection.5.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Document Selection}{59}{section.5.2}\protected@file@percent }
\newlabel{sec:document-selection}{{5.2}{59}{Document Selection}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Unsupervised Methods}{60}{subsection.5.2.1}\protected@file@percent }
\newlabel{subsec:unsupervised-methods}{{5.2.1}{60}{Unsupervised Methods}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{BM25}{60}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Contriever}{60}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Supervised Methods}{61}{subsection.5.2.2}\protected@file@percent }
\newlabel{subsec:supervised-methods}{{5.2.2}{61}{Supervised Methods}{subsection.5.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Jina.ai Reranker}{61}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MS MARCO MiniLM}{62}{subsection.5.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Performance comparison of various distilled MS MARCO models based on BERT architecture, measured across NDCG@10 on TREC DL 2019 and MRR@10 on MS MARCO Dev benchmarks.}}{62}{table.caption.56}\protected@file@percent }
\newlabel{tab:ms-marco-model-comparison}{{5.1}{62}{Performance comparison of various distilled MS MARCO models based on BERT architecture, measured across NDCG@10 on TREC DL 2019 and MRR@10 on MS MARCO Dev benchmarks}{table.caption.56}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Evaluation with Large Language Models}{63}{subsection.5.2.3}\protected@file@percent }
\newlabel{subsec:evaluation-with-large-language-models}{{5.2.3}{63}{Evaluation with Large Language Models}{subsection.5.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Document Retrieval Confusion Matrix based on Jaccard Similarity between documents retrieved by each model.}}{63}{figure.caption.57}\protected@file@percent }
\newlabel{fig:document_retrieval_confusion_matrix}{{5.1}{63}{Document Retrieval Confusion Matrix based on Jaccard Similarity between documents retrieved by each model}{figure.caption.57}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Performance evaluation of various document retrieval methods on the FactBench dataset, using the Gemma2 model.}}{64}{table.caption.58}\protected@file@percent }
\newlabel{tab:evaluation_results}{{5.2}{64}{Performance evaluation of various document retrieval methods on the FactBench dataset, using the Gemma2 model}{table.caption.59}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Embedding Models}{65}{section.5.3}\protected@file@percent }
\newlabel{sec:embedding-models}{{5.3}{65}{Embedding Models}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Gte-large-en-v1.5}{65}{subsection.5.3.1}\protected@file@percent }
\newlabel{subsec:alibaba-nlp}{{5.3.1}{65}{Gte-large-en-v1.5}{subsection.5.3.1}{}}
\AC@undonewlabel{acro:RoPE}
\newlabel{acro:RoPE}{{5.3.1}{65}{Gte-large-en-v1.5}{section*.60}{}}
\acronymused{RoPE}
\AC@undonewlabel{acro:MTEB}
\newlabel{acro:MTEB}{{5.3.1}{66}{Gte-large-en-v1.5}{section*.61}{}}
\acronymused{MTEB}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Jina-embeddings-v3}{66}{subsection.5.3.2}\protected@file@percent }
\newlabel{subsec:jinaai}{{5.3.2}{66}{Jina-embeddings-v3}{subsection.5.3.2}{}}
\acronymused{NLP}
\acronymused{RoPE}
\AC@undonewlabel{acro:LoRA}
\newlabel{acro:LoRA}{{5.3.2}{66}{Jina-embeddings-v3}{section*.62}{}}
\acronymused{LoRA}
\AC@undonewlabel{acro:MRL}
\newlabel{acro:MRL}{{5.3.2}{67}{Jina-embeddings-v3}{section*.63}{}}
\acronymused{MRL}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Stella\_en\_1.5B\_v5}{67}{subsection.5.3.3}\protected@file@percent }
\newlabel{subsec:dunzhang}{{5.3.3}{67}{Stella\_en\_1.5B\_v5}{subsection.5.3.3}{}}
\acronymused{MRL}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Multilingual-e5-large-instruct}{68}{subsection.5.3.4}\protected@file@percent }
\newlabel{subsec:nextcloud-ai}{{5.3.4}{68}{Multilingual-e5-large-instruct}{subsection.5.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.5}bge-small-en-v1.5}{69}{subsection.5.3.5}\protected@file@percent }
\newlabel{subsec:baai}{{5.3.5}{69}{bge-small-en-v1.5}{subsection.5.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.6}Comparative Analysis}{69}{subsection.5.3.6}\protected@file@percent }
\newlabel{subsec:comparative-analysis}{{5.3.6}{69}{Comparative Analysis}{subsection.5.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model Size and Efficiency}{69}{subsection.5.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison of characteristics of embedding models}}{70}{table.caption.64}\protected@file@percent }
\newlabel{tab:comparison-embeddings}{{5.3}{70}{Comparison of characteristics of embedding models}{table.caption.64}{}}
\@writefile{toc}{\contentsline {subsubsection}{Language Coverage}{70}{table.caption.64}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion}{70}{table.caption.64}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Performance evaluation of various embedding models on the FactBench dataset, using the Gemma2 model.}}{71}{table.caption.65}\protected@file@percent }
\newlabel{tab:evaluation_results_embedding}{{5.4}{71}{Performance evaluation of various embedding models on the FactBench dataset, using the Gemma2 model}{table.caption.66}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Chunking Strategies}{72}{section.5.4}\protected@file@percent }
\newlabel{sec:chunking-strategies}{{5.4}{72}{Chunking Strategies}{section.5.4}{}}
\acronymused{RAG}
\acronymused{RAG}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Parsing Documents into Text Chunks}{72}{subsection.5.4.1}\protected@file@percent }
\newlabel{subsec:parsing-documents-into-text-chunks}{{5.4.1}{72}{Parsing Documents into Text Chunks}{subsection.5.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Methodology}{72}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Chunk Sizes}{72}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Smaller Child Chunks Referring to Bigger Parent Chunks (Small2Big)}{72}{subsection.5.4.2}\protected@file@percent }
\newlabel{subsec:smaller-child-chunks-referring-to-bigger-parent-chunks}{{5.4.2}{72}{Smaller Child Chunks Referring to Bigger Parent Chunks (Small2Big)}{subsection.5.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Methodology}{73}{subsection.5.4.2}\protected@file@percent }
\newlabel{lst:small2big_chunking}{{5.1}{73}{Small2Big Chunking Method}{lstlisting.5.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces Small2Big Chunking Method}}{73}{lstlisting.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Sentence Window Retrieval}{73}{subsection.5.4.3}\protected@file@percent }
\newlabel{subsec:sentence-window-retrieval}{{5.4.3}{73}{Sentence Window Retrieval}{subsection.5.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Methodology}{74}{subsection.5.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Node sentence window replacement technique as described by Liu~\blx@tocontentsinit {0}\cite {liu2023tweet}.}}{74}{figure.caption.67}\protected@file@percent }
\newlabel{fig:window-ret}{{5.2}{74}{Node sentence window replacement technique as described by Liu~\cite {liu2023tweet}}{figure.caption.67}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Advantages and Limitations}{74}{subsection.5.4.4}\protected@file@percent }
\newlabel{subsec:advantages-and-limitations}{{5.4.4}{74}{Advantages and Limitations}{subsection.5.4.4}{}}
\gdef \LT@iii {\LT@entry 
    {1}{91.59993pt}\LT@entry 
    {1}{163.32687pt}\LT@entry 
    {1}{163.32687pt}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Advantages and Limitations of different chunking strategies for RAG systems.}}{75}{table.caption.68}\protected@file@percent }
\newlabel{tab:window-segmentation-analysis}{{5.5}{75}{Advantages and Limitations of different chunking strategies for RAG systems}{table.caption.68}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Evaluation}{75}{subsection.5.4.5}\protected@file@percent }
\newlabel{subsec:evaluation}{{5.4.5}{75}{Evaluation}{subsection.5.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Performance evaluation of various chunking strategy on the FactBench dataset, using the Gemma2 model.}}{76}{table.caption.69}\protected@file@percent }
\newlabel{tab:table_chunking}{{5.6}{76}{Performance evaluation of various chunking strategy on the FactBench dataset, using the Gemma2 model}{table.caption.69}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Similarity Cut-off}{76}{section.5.5}\protected@file@percent }
\newlabel{sec:similar-cut-off}{{5.5}{76}{Similarity Cut-off}{section.5.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Similarity Cutoff Postprocessor (re-rank score)}}{76}{algorithm.3}\protected@file@percent }
\newlabel{alg:algorithm}{{3}{76}{Similarity Cutoff Postprocessor (re-rank score)}{algorithm.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Performance evaluation of similarity cut-off method on the FactBench dataset, using the Gemma2 model.}}{77}{table.caption.70}\protected@file@percent }
\newlabel{tab:table_similarity_cut}{{5.7}{77}{Performance evaluation of similarity cut-off method on the FactBench dataset, using the Gemma2 model}{table.caption.71}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Top K}{77}{section.5.6}\protected@file@percent }
\newlabel{sec:top-k}{{5.6}{77}{Top K}{section.5.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Performance evaluation of different Top\_k retrieval strategies on the FactBench dataset using the Gemma2 model. }}{77}{table.caption.72}\protected@file@percent }
\newlabel{tab:table_top_k}{{5.8}{77}{Performance evaluation of different Top\_k retrieval strategies on the FactBench dataset using the Gemma2 model}{table.caption.73}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Evaluation}{78}{section.5.7}\protected@file@percent }
\newlabel{sec:evaluation-and-discussion}{{5.7}{78}{Evaluation}{section.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Category-wise performance of different models in identifying Positive Labels (left) and Negative Labels (right) on the FactBench dataset.}}{78}{figure.caption.74}\protected@file@percent }
\newlabel{fig:radar-charts}{{5.3}{78}{Category-wise performance of different models in identifying Positive Labels (left) and Negative Labels (right) on the FactBench dataset}{figure.caption.74}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Category-wise performance evaluation results of various models on the FactBench dataset.}}{79}{table.caption.75}\protected@file@percent }
\newlabel{tab:evaluation_results-full-category}{{5.9}{79}{Category-wise performance evaluation results of various models on the FactBench dataset}{table.caption.75}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Performance evaluation of various models on the FactBench dataset.}}{79}{table.caption.76}\protected@file@percent }
\newlabel{tab:evaluation_results-full-wo-category}{{5.10}{79}{Performance evaluation of various models on the FactBench dataset}{table.caption.77}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Failure Analysis}{79}{section.5.8}\protected@file@percent }
\newlabel{sec:faiure-analysis}{{5.8}{79}{Failure Analysis}{section.5.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Error Type Categorization}{79}{section.5.8}\protected@file@percent }
\gdef \LT@iv {\LT@entry 
    {1}{97.35826pt}\LT@entry 
    {1}{97.35826pt}\LT@entry 
    {1}{223.53716pt}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Example of failure cases and error analysis observed in the FactBench dataset using generated results and explanations.}}{80}{table.caption.78}\protected@file@percent }
\newlabel{tab:factbench-failure-analysis}{{5.11}{80}{Example of failure cases and error analysis observed in the FactBench dataset using generated results and explanations}{table.caption.78}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Prediction accuracy on the FactBench dataset, focusing on incorrect predictions. The right chart illustrates the Distribution of Fully Incorrect Predictions (4/4), detailing the instances where all predictions made by the models were incorrect. The left chart depicts the Distribution of Partially Incorrect Predictions (3/4).}}{81}{figure.caption.79}\protected@file@percent }
\newlabel{fig:wrong_prediction_distribution}{{5.4}{81}{Prediction accuracy on the FactBench dataset, focusing on incorrect predictions. The right chart illustrates the Distribution of Fully Incorrect Predictions (4/4), detailing the instances where all predictions made by the models were incorrect. The left chart depicts the Distribution of Partially Incorrect Predictions (3/4)}{figure.caption.79}{}}
\@setckpt{chapters/05_ablation}{
\setcounter{page}{82}
\setcounter{equation}{3}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{7}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{11}
\setcounter{section@level}{0}
\setcounter{Item}{17}
\setcounter{Hfootnote}{26}
\setcounter{bookmark@seq@number}{88}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{12}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{18}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{58}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{2}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{r@tfl@t}{0}
\setcounter{LT@tables}{4}
\setcounter{LT@chunks}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{lstlisting}{1}
}

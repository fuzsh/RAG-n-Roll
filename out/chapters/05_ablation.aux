\relax 
\providecommand\hyper@newdestlabel[2]{}
\@writefile{toc}{\contentsline {chapter}{\numberline {5}Ablation Study}{53}{chapter.5}\protected@file@percent }
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{ch:ablation}{{5}{53}{Ablation Study}{chapter.5}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.1}Evaluation Methodology}{54}{section.5.1}\protected@file@percent }
\newlabel{sec:evaluation-methodology}{{5.1}{54}{Evaluation Methodology}{section.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.1}Iterative Optimization Process}{54}{subsection.5.1.1}\protected@file@percent }
\newlabel{subsec:iterative-optimization-process}{{5.1.1}{54}{Iterative Optimization Process}{subsection.5.1.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.2}Evaluation Metrics}{54}{subsection.5.1.2}\protected@file@percent }
\newlabel{subsec:ablation-metrics}{{5.1.2}{54}{Evaluation Metrics}{subsection.5.1.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1.3}Significance of the Methodology}{55}{subsection.5.1.3}\protected@file@percent }
\newlabel{subsec:significance-of-the-methodology}{{5.1.3}{55}{Significance of the Methodology}{subsection.5.1.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.2}Document Selection}{55}{section.5.2}\protected@file@percent }
\newlabel{sec:document-selection}{{5.2}{55}{Document Selection}{section.5.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.1}Unsupervised Methods}{56}{subsection.5.2.1}\protected@file@percent }
\newlabel{subsec:unsupervised-methods}{{5.2.1}{56}{Unsupervised Methods}{subsection.5.2.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{BM25}{56}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Contriever}{56}{subsection.5.2.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.2}Supervised Methods}{57}{subsection.5.2.2}\protected@file@percent }
\newlabel{subsec:supervised-methods}{{5.2.2}{57}{Supervised Methods}{subsection.5.2.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Jina.ai Reranker}{57}{subsection.5.2.2}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{MS MARCO MiniLM}{58}{subsection.5.2.2}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.1}{\ignorespaces Performance comparison of various distilled MS MARCO models based on BERT architecture, measured across NDCG@10 on TREC DL 2019 and MRR@10 on MS MARCO Dev benchmarks.}}{58}{table.caption.48}\protected@file@percent }
\newlabel{tab:ms-marco-model-comparison}{{5.1}{58}{Performance comparison of various distilled MS MARCO models based on BERT architecture, measured across NDCG@10 on TREC DL 2019 and MRR@10 on MS MARCO Dev benchmarks}{table.caption.48}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2.3}Evaluation with Large Language Models}{59}{subsection.5.2.3}\protected@file@percent }
\newlabel{subsec:evaluation-with-large-language-models}{{5.2.3}{59}{Evaluation with Large Language Models}{subsection.5.2.3}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.1}{\ignorespaces Document Retrieval Confusion Matrix based on Jaccard Similarity between documents retrieved by each model.}}{59}{figure.caption.49}\protected@file@percent }
\newlabel{fig:document_retrieval_confusion_matrix}{{5.1}{59}{Document Retrieval Confusion Matrix based on Jaccard Similarity between documents retrieved by each model}{figure.caption.49}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.2}{\ignorespaces Performance evaluation of various document retrieval methods on the FactBench dataset, using the Gemma2 model.}}{60}{table.caption.50}\protected@file@percent }
\newlabel{tab:evaluation_results}{{5.2}{60}{Performance evaluation of various document retrieval methods on the FactBench dataset, using the Gemma2 model}{table.caption.51}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.3}Embedding Models}{61}{section.5.3}\protected@file@percent }
\newlabel{sec:embedding-models}{{5.3}{61}{Embedding Models}{section.5.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.1}Gte-large-en-v1.5}{61}{subsection.5.3.1}\protected@file@percent }
\newlabel{subsec:alibaba-nlp}{{5.3.1}{61}{Gte-large-en-v1.5}{subsection.5.3.1}{}}
\AC@undonewlabel{acro:RoPE}
\newlabel{acro:RoPE}{{5.3.1}{61}{Gte-large-en-v1.5}{section*.52}{}}
\acronymused{RoPE}
\AC@undonewlabel{acro:MTEB}
\newlabel{acro:MTEB}{{5.3.1}{62}{Gte-large-en-v1.5}{section*.53}{}}
\acronymused{MTEB}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.2}Jina-embeddings-v3}{62}{subsection.5.3.2}\protected@file@percent }
\newlabel{subsec:jinaai}{{5.3.2}{62}{Jina-embeddings-v3}{subsection.5.3.2}{}}
\acronymused{NLP}
\acronymused{RoPE}
\AC@undonewlabel{acro:LoRA}
\newlabel{acro:LoRA}{{5.3.2}{62}{Jina-embeddings-v3}{section*.54}{}}
\acronymused{LoRA}
\AC@undonewlabel{acro:MRL}
\newlabel{acro:MRL}{{5.3.2}{63}{Jina-embeddings-v3}{section*.55}{}}
\acronymused{MRL}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.3}Stella\_en\_1.5B\_v5}{63}{subsection.5.3.3}\protected@file@percent }
\newlabel{subsec:dunzhang}{{5.3.3}{63}{Stella\_en\_1.5B\_v5}{subsection.5.3.3}{}}
\acronymused{MRL}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.4}Multilingual-e5-large-instruct}{64}{subsection.5.3.4}\protected@file@percent }
\newlabel{subsec:nextcloud-ai}{{5.3.4}{64}{Multilingual-e5-large-instruct}{subsection.5.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.5}bge-small-en-v1.5}{65}{subsection.5.3.5}\protected@file@percent }
\newlabel{subsec:baai}{{5.3.5}{65}{bge-small-en-v1.5}{subsection.5.3.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3.6}Comparative Analysis}{65}{subsection.5.3.6}\protected@file@percent }
\newlabel{subsec:comparative-analysis}{{5.3.6}{65}{Comparative Analysis}{subsection.5.3.6}{}}
\@writefile{toc}{\contentsline {subsubsection}{Model Size and Efficiency}{65}{subsection.5.3.6}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison of characteristics of embedding models}}{66}{table.caption.56}\protected@file@percent }
\newlabel{tab:comparison-embeddings}{{5.3}{66}{Comparison of characteristics of embedding models}{table.caption.56}{}}
\@writefile{toc}{\contentsline {subsubsection}{Language Coverage}{66}{table.caption.56}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Conclusion}{66}{table.caption.56}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {5.4}{\ignorespaces Performance evaluation of various embedding models on the FactBench dataset, using the Gemma2 model.}}{67}{table.caption.57}\protected@file@percent }
\newlabel{tab:evaluation_results_embedding}{{5.4}{67}{Performance evaluation of various embedding models on the FactBench dataset, using the Gemma2 model}{table.caption.58}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.4}Chunking Strategies}{68}{section.5.4}\protected@file@percent }
\newlabel{sec:chunking-strategies}{{5.4}{68}{Chunking Strategies}{section.5.4}{}}
\acronymused{RAG}
\acronymused{RAG}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.1}Parsing Documents into Text Chunks}{68}{subsection.5.4.1}\protected@file@percent }
\newlabel{subsec:parsing-documents-into-text-chunks}{{5.4.1}{68}{Parsing Documents into Text Chunks}{subsection.5.4.1}{}}
\@writefile{toc}{\contentsline {subsubsection}{Methodology}{68}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsubsection}{Chunk Sizes}{68}{subsection.5.4.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.2}Smaller Child Chunks Referring to Bigger Parent Chunks (Small2Big)}{68}{subsection.5.4.2}\protected@file@percent }
\newlabel{subsec:smaller-child-chunks-referring-to-bigger-parent-chunks}{{5.4.2}{68}{Smaller Child Chunks Referring to Bigger Parent Chunks (Small2Big)}{subsection.5.4.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{Methodology}{69}{subsection.5.4.2}\protected@file@percent }
\newlabel{lst:small2big_chunking}{{5.1}{69}{Small2Big Chunking Method}{lstlisting.5.1}{}}
\@writefile{lol}{\contentsline {lstlisting}{\numberline {5.1}{\ignorespaces Small2Big Chunking Method}}{69}{lstlisting.5.1}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.3}Sentence Window Retrieval}{69}{subsection.5.4.3}\protected@file@percent }
\newlabel{subsec:sentence-window-retrieval}{{5.4.3}{69}{Sentence Window Retrieval}{subsection.5.4.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{Methodology}{70}{subsection.5.4.3}\protected@file@percent }
\@writefile{lof}{\contentsline {figure}{\numberline {5.2}{\ignorespaces Node sentence window replacement technique as described by Liu~\blx@tocontentsinit {0}\cite {liu2023tweet}.}}{70}{figure.caption.59}\protected@file@percent }
\newlabel{fig:window-ret}{{5.2}{70}{Node sentence window replacement technique as described by Liu~\cite {liu2023tweet}}{figure.caption.59}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.4}Advantages and Limitations}{70}{subsection.5.4.4}\protected@file@percent }
\newlabel{subsec:advantages-and-limitations}{{5.4.4}{70}{Advantages and Limitations}{subsection.5.4.4}{}}
\gdef \LT@iii {\LT@entry 
    {1}{91.59993pt}\LT@entry 
    {1}{163.32687pt}\LT@entry 
    {1}{163.32687pt}}
\@writefile{lot}{\contentsline {table}{\numberline {5.5}{\ignorespaces Advantages and Limitations of different chunking strategies for RAG systems.}}{71}{table.caption.60}\protected@file@percent }
\newlabel{tab:window-segmentation-analysis}{{5.5}{71}{Advantages and Limitations of different chunking strategies for RAG systems}{table.caption.60}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.4.5}Evaluation}{71}{subsection.5.4.5}\protected@file@percent }
\newlabel{subsec:evaluation}{{5.4.5}{71}{Evaluation}{subsection.5.4.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.6}{\ignorespaces Performance evaluation of various chunking strategy on the FactBench dataset, using the Gemma2 model.}}{72}{table.caption.61}\protected@file@percent }
\newlabel{tab:table_chunking}{{5.6}{72}{Performance evaluation of various chunking strategy on the FactBench dataset, using the Gemma2 model}{table.caption.61}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.5}Similarity Cut-off}{72}{section.5.5}\protected@file@percent }
\newlabel{sec:similar-cut-off}{{5.5}{72}{Similarity Cut-off}{section.5.5}{}}
\@writefile{loa}{\contentsline {algorithm}{\numberline {3}{\ignorespaces Similarity Cutoff Postprocessor (re-rank score)}}{72}{algorithm.3}\protected@file@percent }
\newlabel{alg:algorithm}{{3}{72}{Similarity Cutoff Postprocessor (re-rank score)}{algorithm.3}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.7}{\ignorespaces Performance evaluation of similarity cut-off method on the FactBench dataset, using the Gemma2 model.}}{73}{table.caption.62}\protected@file@percent }
\newlabel{tab:table_similarity_cut}{{5.7}{73}{Performance evaluation of similarity cut-off method on the FactBench dataset, using the Gemma2 model}{table.caption.63}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.6}Top K}{73}{section.5.6}\protected@file@percent }
\newlabel{sec:top-k}{{5.6}{73}{Top K}{section.5.6}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.8}{\ignorespaces Performance evaluation of different Top\_k retrieval strategies on the FactBench dataset using the Gemma2 model. }}{73}{table.caption.64}\protected@file@percent }
\newlabel{tab:table_top_k}{{5.8}{73}{Performance evaluation of different Top\_k retrieval strategies on the FactBench dataset using the Gemma2 model}{table.caption.65}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.7}Evaluation}{74}{section.5.7}\protected@file@percent }
\newlabel{sec:evaluation-and-discussion}{{5.7}{74}{Evaluation}{section.5.7}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.3}{\ignorespaces Category-wise performance of different models in identifying Positive Labels (left) and Negative Labels (right) on the FactBench dataset.}}{74}{figure.caption.66}\protected@file@percent }
\newlabel{fig:radar-charts}{{5.3}{74}{Category-wise performance of different models in identifying Positive Labels (left) and Negative Labels (right) on the FactBench dataset}{figure.caption.66}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.9}{\ignorespaces Category-wise performance evaluation results of various models on the FactBench dataset.}}{75}{table.caption.67}\protected@file@percent }
\newlabel{tab:evaluation_results-full-category}{{5.9}{75}{Category-wise performance evaluation results of various models on the FactBench dataset}{table.caption.67}{}}
\@writefile{lot}{\contentsline {table}{\numberline {5.10}{\ignorespaces Performance evaluation of various models on the FactBench dataset.}}{75}{table.caption.68}\protected@file@percent }
\newlabel{tab:evaluation_results-full-wo-category}{{5.10}{75}{Performance evaluation of various models on the FactBench dataset}{table.caption.68}{}}
\@writefile{toc}{\contentsline {section}{\numberline {5.8}Failure Analysis}{75}{section.5.8}\protected@file@percent }
\newlabel{sec:faiure-analysis}{{5.8}{75}{Failure Analysis}{section.5.8}{}}
\@writefile{toc}{\contentsline {subsubsection}{Error Type Categorization}{75}{section.5.8}\protected@file@percent }
\gdef \LT@iv {\LT@entry 
    {1}{97.35826pt}\LT@entry 
    {1}{97.35826pt}\LT@entry 
    {1}{223.53716pt}}
\@writefile{lot}{\contentsline {table}{\numberline {5.11}{\ignorespaces Example of failure cases and error analysis observed in the FactBench dataset using generated results and explanations.}}{76}{table.caption.69}\protected@file@percent }
\newlabel{tab:factbench-failure-analysis}{{5.11}{76}{Example of failure cases and error analysis observed in the FactBench dataset using generated results and explanations}{table.caption.69}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5.4}{\ignorespaces Prediction accuracy on the FactBench dataset, focusing on incorrect predictions. The left chart illustrates the Distribution of Fully Incorrect Predictions (4/4), detailing the instances where all predictions made by the model were incorrect. The right chart depicts the Distribution of Partially Incorrect Predictions (3/4).}}{77}{figure.caption.70}\protected@file@percent }
\newlabel{fig:wrong_prediction_distribution}{{5.4}{77}{Prediction accuracy on the FactBench dataset, focusing on incorrect predictions. The left chart illustrates the Distribution of Fully Incorrect Predictions (4/4), detailing the instances where all predictions made by the model were incorrect. The right chart depicts the Distribution of Partially Incorrect Predictions (3/4)}{figure.caption.70}{}}
\@setckpt{chapters/05_ablation}{
\setcounter{page}{78}
\setcounter{equation}{3}
\setcounter{enumi}{4}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{7}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{5}
\setcounter{section}{8}
\setcounter{subsection}{0}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{4}
\setcounter{table}{11}
\setcounter{section@level}{0}
\setcounter{Item}{17}
\setcounter{Hfootnote}{25}
\setcounter{bookmark@seq@number}{89}
\setcounter{parentequation}{0}
\setcounter{caption@flags}{2}
\setcounter{continuedfloat}{0}
\setcounter{subfigure}{0}
\setcounter{subtable}{0}
\setcounter{float@type}{16}
\setcounter{algorithm}{3}
\setcounter{ALG@line}{12}
\setcounter{ALG@rem}{0}
\setcounter{ALG@nested}{0}
\setcounter{ALG@Lnr}{2}
\setcounter{ALG@blocknr}{10}
\setcounter{ALG@storecount}{0}
\setcounter{ALG@tmpcounter}{0}
\setcounter{lstnumber}{18}
\setcounter{tabx@nest}{0}
\setcounter{listtotal}{0}
\setcounter{listcount}{0}
\setcounter{liststart}{0}
\setcounter{liststop}{0}
\setcounter{citecount}{0}
\setcounter{citetotal}{0}
\setcounter{multicitecount}{0}
\setcounter{multicitetotal}{0}
\setcounter{instcount}{57}
\setcounter{maxnames}{3}
\setcounter{minnames}{1}
\setcounter{maxitems}{3}
\setcounter{minitems}{1}
\setcounter{citecounter}{0}
\setcounter{maxcitecounter}{0}
\setcounter{savedcitecounter}{0}
\setcounter{uniquelist}{0}
\setcounter{uniquename}{0}
\setcounter{refsection}{0}
\setcounter{refsegment}{0}
\setcounter{maxextratitle}{0}
\setcounter{maxextratitleyear}{0}
\setcounter{maxextraname}{2}
\setcounter{maxextradate}{0}
\setcounter{maxextraalpha}{0}
\setcounter{abbrvpenalty}{50}
\setcounter{highnamepenalty}{50}
\setcounter{lownamepenalty}{25}
\setcounter{maxparens}{3}
\setcounter{parenlevel}{0}
\setcounter{blx@maxsection}{0}
\setcounter{mincomprange}{10}
\setcounter{maxcomprange}{100000}
\setcounter{mincompwidth}{1}
\setcounter{afterword}{0}
\setcounter{savedafterword}{0}
\setcounter{annotator}{0}
\setcounter{savedannotator}{0}
\setcounter{author}{0}
\setcounter{savedauthor}{0}
\setcounter{bookauthor}{0}
\setcounter{savedbookauthor}{0}
\setcounter{commentator}{0}
\setcounter{savedcommentator}{0}
\setcounter{editor}{0}
\setcounter{savededitor}{0}
\setcounter{editora}{0}
\setcounter{savededitora}{0}
\setcounter{editorb}{0}
\setcounter{savededitorb}{0}
\setcounter{editorc}{0}
\setcounter{savededitorc}{0}
\setcounter{foreword}{0}
\setcounter{savedforeword}{0}
\setcounter{holder}{0}
\setcounter{savedholder}{0}
\setcounter{introduction}{0}
\setcounter{savedintroduction}{0}
\setcounter{namea}{0}
\setcounter{savednamea}{0}
\setcounter{nameb}{0}
\setcounter{savednameb}{0}
\setcounter{namec}{0}
\setcounter{savednamec}{0}
\setcounter{translator}{0}
\setcounter{savedtranslator}{0}
\setcounter{shortauthor}{0}
\setcounter{savedshortauthor}{0}
\setcounter{shorteditor}{0}
\setcounter{savedshorteditor}{0}
\setcounter{labelname}{0}
\setcounter{savedlabelname}{0}
\setcounter{institution}{0}
\setcounter{savedinstitution}{0}
\setcounter{lista}{0}
\setcounter{savedlista}{0}
\setcounter{listb}{0}
\setcounter{savedlistb}{0}
\setcounter{listc}{0}
\setcounter{savedlistc}{0}
\setcounter{listd}{0}
\setcounter{savedlistd}{0}
\setcounter{liste}{0}
\setcounter{savedliste}{0}
\setcounter{listf}{0}
\setcounter{savedlistf}{0}
\setcounter{location}{0}
\setcounter{savedlocation}{0}
\setcounter{organization}{0}
\setcounter{savedorganization}{0}
\setcounter{origlocation}{0}
\setcounter{savedoriglocation}{0}
\setcounter{origpublisher}{0}
\setcounter{savedorigpublisher}{0}
\setcounter{publisher}{0}
\setcounter{savedpublisher}{0}
\setcounter{language}{0}
\setcounter{savedlanguage}{0}
\setcounter{origlanguage}{0}
\setcounter{savedoriglanguage}{0}
\setcounter{pageref}{0}
\setcounter{savedpageref}{0}
\setcounter{textcitecount}{0}
\setcounter{textcitetotal}{0}
\setcounter{textcitemaxnames}{0}
\setcounter{biburlbigbreakpenalty}{100}
\setcounter{biburlbreakpenalty}{200}
\setcounter{biburlnumpenalty}{0}
\setcounter{biburlucpenalty}{0}
\setcounter{biburllcpenalty}{0}
\setcounter{smartand}{1}
\setcounter{bbx:relatedcount}{0}
\setcounter{bbx:relatedtotal}{0}
\setcounter{FancyVerbLine}{0}
\setcounter{r@tfl@t}{0}
\setcounter{LT@tables}{4}
\setcounter{LT@chunks}{1}
\setcounter{@pps}{0}
\setcounter{@ppsavesec}{0}
\setcounter{@ppsaveapp}{0}
\setcounter{lstlisting}{1}
}

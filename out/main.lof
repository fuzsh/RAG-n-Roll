\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Distinctions between human and LLM Inferences. The entailment prediction performance of humans and LLMs are depicted by a 5-star rating scale~\blx@tocontentsinit {0}\cite {sanyal2024machinesbettercomplexreasoning}.}}{8}{figure.caption.21}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Comparison of claim verification systems between NLP-based (traditional) and LLM-based for claim veracity.~\blx@tocontentsinit {0}\cite {dmonte2024claimverificationagelarge}.}}{10}{figure.caption.22}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces The proposed RAFTS~\blx@tocontentsinit {0}\cite {yue2024retrievalaugmentedfactverification}, which performs few-shot fact verification by incorporating informative in-context demonstrations and contrastive arguments with nuanced information derived from the retrieved documents}}{12}{figure.caption.23}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces An overview of the fact-checking pipeline contrasting the baseline Sub-Question Generation approach from the Chain of RAG and Tree of RAG approach followed by veracity prediction and explanation.}}{13}{figure.caption.24}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces RAG-Based Fact Verification Pipeline}}{16}{figure.caption.25}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Cross-Encoder component, which assesses the relevance of generated questions by taking multiple input pairs and assigning a relevance score to each, supporting question evaluation and refinement.}}{20}{figure.caption.27}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Knowledge Distillation Process for Enhanced Re-Ranking Efficiency}}{21}{figure.caption.28}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Fetching results from Google Search engine for top $N$ questions and the main knowledge graph query.}}{23}{figure.caption.30}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Extracted content from the crawled URLs using newspaper4k library.}}{25}{figure.caption.32}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Partition-wise Model Performance Comparison: Accuracy and F1-scores for knowledge graph fact verification on DBpedia dataset. Gray bars indicate stratum weights (log scale).}}{52}{figure.caption.52}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Collecting logs and leveraging LLM-generated reasoning, combined with contextual document embeddings (jxm/cde-small-v1), to cluster errors using a hierarchical density-based spatial technique.}}{53}{figure.caption.53}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Model Overlap Heatmaps by Category and Dataset. Each cell shows the percentage overlap in errors between model pairs. Matrices are organized by error category (UnLabeled, Relationship, Role Errors, etc.) and dataset (DBpedia, FactBench, YAGO), revealing patterns in how models agree or disagree when making verification errors."}}{55}{figure.caption.56}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Normalized distribution of error clusters across datasets.}}{56}{figure.caption.57}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Distribution of error clusters across selected LLMs.}}{56}{figure.caption.57}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Distribution of tendency to be wrong across gemma2, qwen2.5, \textit {LLama3.1} and mistral models. The right chart illustrates the distribution of fully incorrect predictions (4/4) detailing the instances where all predictions made by the models were incorrect. The left chart depicts the distribution of just one wrong predictions (1/4).}}{56}{figure.caption.58}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Error Distribution Analysis Across Different Language Models and Frequency Strata.}}{58}{figure.caption.59}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces Distribution of Error Categories Across Different Language Models and Frequency Strata}}{59}{figure.caption.60}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Document Retrieval Confusion Matrix based on Jaccard Similarity between documents retrieved by each model.}}{67}{figure.caption.62}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Node sentence window replacement technique as described by Liu~\blx@tocontentsinit {0}\cite {liu2023tweet}.}}{78}{figure.caption.72}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Category-wise performance of different models in identifying Positive Labels (left) and Negative Labels (right) on the FactBench dataset.}}{82}{figure.caption.79}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Prediction accuracy on the FactBench dataset, focusing on incorrect predictions. The right chart illustrates the Distribution of Fully Incorrect Predictions (4/4), detailing the instances where all predictions made by the models were incorrect. The left chart depicts the Distribution of Partially Incorrect Predictions (3/4).}}{85}{figure.caption.84}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }

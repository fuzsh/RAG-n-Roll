\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\contentsline {figure}{\numberline {1.1}{\ignorespaces Overview of the FactCheck system for fact verification. The system processes a knowledge graph triple through multiple stages: (1) \ac {KG} humanization, converting structured data into natural language; (2) Generation of aspect-specific questions to probe the fact; (3) Google search retrieval based on generated questions; (4) Analysis of retrieved documents containing relevant information; (5) Deployment of multiple \ac {LLMs} as open-source fact-checkers; (6) Final fact verification through majority voting; and (7) tie-breaker module.}}{3}{figure.caption.20}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {2.1}{\ignorespaces Distinctions between human and LLM Inferences. The entailment prediction performance of humans and LLMs are depicted by a 5-star rating scale~\blx@tocontentsinit {0}\cite {sanyal2024machinesbettercomplexreasoning}.}}{10}{figure.caption.24}%
\contentsline {figure}{\numberline {2.2}{\ignorespaces Comparison of claim verification systems between NLP-based (traditional) and LLM-based for claim veracity~\blx@tocontentsinit {0}\cite {dmonte2024claimverificationagelarge}.}}{12}{figure.caption.25}%
\contentsline {figure}{\numberline {2.3}{\ignorespaces The proposed RAFTS~\blx@tocontentsinit {0}\cite {yue2024retrievalaugmentedfactverification}, which performs few-shot fact verification by incorporating informative in-context demonstrations and contrastive arguments with nuanced information derived from the retrieved documents}}{14}{figure.caption.26}%
\contentsline {figure}{\numberline {2.4}{\ignorespaces An overview of the fact-checking pipeline contrasting the baseline Sub-Question Generation approach from the Chain of RAG and Tree of RAG approach followed by veracity prediction and explanation.}}{16}{figure.caption.27}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {3.1}{\ignorespaces FactCheck: RAG-based KG fact verification system}}{20}{figure.caption.28}%
\contentsline {figure}{\numberline {3.2}{\ignorespaces Cross-Encoder component, which assesses the similarity of generated questions by taking multiple input pairs and assigning a relevance score to each, supporting question evaluation and refinement.}}{25}{figure.caption.32}%
\contentsline {figure}{\numberline {3.3}{\ignorespaces Knowledge Distillation Process for Enhanced Re-Ranking Efficiency}}{26}{figure.caption.33}%
\contentsline {figure}{\numberline {3.4}{\ignorespaces Fetching results from Google Search engine for top $N$ questions and the main \ac {KG} query.}}{28}{figure.caption.35}%
\contentsline {figure}{\numberline {3.5}{\ignorespaces Extracted content from the crawled URLs using newspaper4k library.}}{31}{figure.caption.37}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {4.1}{\ignorespaces Collecting logs and leveraging LLM-generated reasoning, combined with contextual document embeddings (jxm/cde-small-v1)~\blx@tocontentsinit {0}\cite {morris2024contextualdocumentembeddings}, to cluster errors using a hierarchical density-based spatial technique.}}{58}{figure.caption.56}%
\contentsline {figure}{\numberline {4.2}{\ignorespaces Model overlap heatmaps by category and dataset. Each cell shows the percentage overlap in errors between model pairs. Matrices are organized by error category (UnLabeled, Relationship, Role Errors, etc.) and dataset (DBpedia, FactBench, YAGO), revealing patterns in how models agree or disagree when making verification errors.}}{60}{figure.caption.59}%
\contentsline {figure}{\numberline {4.3}{\ignorespaces Normalized distribution of error clusters across datasets.}}{61}{figure.caption.60}%
\contentsline {figure}{\numberline {4.4}{\ignorespaces Distribution of error clusters across selected LLMs.}}{61}{figure.caption.60}%
\contentsline {figure}{\numberline {4.5}{\ignorespaces Distribution of tendency to be wrong across gemma2, qwen2.5, LLama3.1 and mistral models. The right chart illustrates the distribution of fully incorrect predictions (4/4) detailing the instances where all predictions made by the models were incorrect. The left chart depicts the distribution of just one wrong predictions (1/4).}}{61}{figure.caption.61}%
\contentsline {figure}{\numberline {4.6}{\ignorespaces Partition-wise model performance comparison: Accuracy and F1-scores for knowledge graph fact verification on DBpedia dataset. Gray bars indicate stratum weights (log scale).}}{63}{figure.caption.63}%
\contentsline {figure}{\numberline {4.7}{\ignorespaces Error distribution analysis across different language models and frequency strata.}}{64}{figure.caption.64}%
\contentsline {figure}{\numberline {4.8}{\ignorespaces distribution of error categories across different language models and frequency strata}}{65}{figure.caption.65}%
\contentsline {figure}{\numberline {4.9}{\ignorespaces Comparative analysis of language model error rates across knowledge domains. (Left) domain-specific error ratesacross 13 knowledge categories, with overlaid sample distribution bars. (Right) distribution of error rates for each model}}{66}{figure.caption.66}%
\addvspace {10\p@ }
\contentsline {figure}{\numberline {5.1}{\ignorespaces Document retrieval confusion matrix based on Jaccard similarity between documents retrieved by each model.}}{75}{figure.caption.68}%
\contentsline {figure}{\numberline {5.2}{\ignorespaces Node sentence window replacement technique as described by Liu~\blx@tocontentsinit {0}\cite {liu2023tweet}.}}{86}{figure.caption.78}%
\contentsline {figure}{\numberline {5.3}{\ignorespaces Category-wise performance of different models in identifying Positive Labels (left) and Negative Labels (right) on the FactBench dataset.}}{90}{figure.caption.85}%
\contentsline {figure}{\numberline {5.4}{\ignorespaces Prediction accuracy on the FactBench dataset, focusing on incorrect predictions. The right chart illustrates the Distribution of Fully Incorrect Predictions (4/4), detailing the instances where all predictions made by the models were incorrect. The left chart depicts the Distribution of Partially Incorrect Predictions (3/4).}}{93}{figure.caption.90}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }

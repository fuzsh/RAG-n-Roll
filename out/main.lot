\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Examples of human-understandable text generation, illustrates entries from multiple \acp {KG}, detailing the transformation of raw triples into readable sentences.}}{24}{table.caption.29}%
\contentsline {table}{\numberline {3.2}{\ignorespaces Example of generated questions by the Gemma2 model with relevance scores assigned by the Jina Re-ranker Cross-Encoder.}}{27}{table.caption.32}%
\contentsline {table}{\numberline {3.3}{\ignorespaces Performance of our LLM-based tasks in production, generated by Openlit with Gemma2 model.}}{40}{table.caption.36}%
\contentsline {table}{\numberline {3.4}{\ignorespaces Performance of our information retrieval mechanisms.}}{40}{table.caption.38}%
\contentsline {table}{\numberline {3.5}{\ignorespaces Comprenhensive list of decision points in the system flow.}}{41}{table.caption.40}%
\contentsline {table}{\numberline {3.6}{\ignorespaces Limitations of the system, categorized by scope of knowledge.}}{43}{table.caption.41}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Statistical summary of FactBench, YAGO, and DBpedia datasets}}{48}{table.caption.42}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Summary of key strengths of selected candidate LLMs for knowledge graph fact verification.}}{52}{table.caption.48}%
\contentsline {table}{\numberline {4.3}{\ignorespaces System configurations for empirical evaluation}}{54}{table.4.3}%
\contentsline {table}{\numberline {4.4}{\ignorespaces Empirical evaluation results of the proposed system and candidate LLMs over the FactBench, YAGO, and DBpedia.}}{56}{table.caption.49}%
\contentsline {table}{\numberline {4.5}{\ignorespaces Statistical analysis of output tokens and request times per query across FactBench, YAGO, and DBpedia datasets for each used model.}}{56}{table.caption.51}%
\contentsline {table}{\numberline {4.6}{\ignorespaces Statistical analysis of request time per query across FactBench, YAGO, and DBpedia datasets.}}{57}{table.caption.52}%
\contentsline {table}{\numberline {4.7}{\ignorespaces Dataset-wise error clustering based on LLM-generated reasoning, using Contextual Document Embeddings for embeddings, UMAP, and HDBSCAN.}}{60}{table.caption.55}%
\contentsline {table}{\numberline {4.8}{\ignorespaces Partition-wise evaluation results of the proposed system and candidate LLMs over the DBpedia dataset.}}{63}{table.caption.60}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Performance comparison of various distilled MS MARCO models based on BERT architecture, measured across NDCG@10 on TREC DL 2019 and MRR@10 on MS MARCO Dev benchmarks.}}{74}{table.caption.65}%
\contentsline {table}{\numberline {5.2}{\ignorespaces Performance evaluation of various document retrieval methods on the FactBench dataset, using the Gemma2 model.}}{76}{table.caption.67}%
\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison of characteristics of embedding models}}{82}{table.caption.73}%
\contentsline {table}{\numberline {5.4}{\ignorespaces Performance evaluation of various embedding models on the FactBench dataset, using the Gemma2 model.}}{83}{table.caption.74}%
\contentsline {table}{\numberline {5.5}{\ignorespaces Advantages and Limitations of different chunking strategies for RAG systems.}}{87}{table.caption.77}%
\contentsline {table}{\numberline {5.6}{\ignorespaces Performance evaluation of various chunking strategy on the FactBench dataset, using the Gemma2 model.}}{88}{table.caption.78}%
\contentsline {table}{\numberline {5.7}{\ignorespaces Performance evaluation of similarity cut-off method on the FactBench dataset, using the Gemma2 model.}}{89}{table.caption.79}%
\contentsline {table}{\numberline {5.8}{\ignorespaces Performance evaluation of different Top\_k retrieval strategies on the FactBench dataset using the Gemma2 model. }}{89}{table.caption.81}%
\contentsline {table}{\numberline {5.9}{\ignorespaces Category-wise performance evaluation results of various models on the FactBench dataset.}}{91}{table.caption.84}%
\contentsline {table}{\numberline {5.10}{\ignorespaces Performance evaluation of various models on the FactBench dataset.}}{91}{table.caption.85}%
\contentsline {table}{\numberline {5.11}{\ignorespaces Example of failure cases and error analysis observed in the FactBench dataset using generated results and explanations.}}{92}{table.caption.87}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {B.1}{\ignorespaces Evaluation of text segmentation using a chunk Size of 512, text chunks derived from the entry "Henry Dunant award Nobel Peace Prize".}}{102}{table.caption.90}%
\contentsline {table}{\numberline {B.2}{\ignorespaces Evaluation of text segmentation using a Small to Big technique (base chunk size 1024), text chunks derived from the entry "Henry Dunant award Nobel Peace Prize".}}{103}{table.caption.91}%
\contentsline {table}{\numberline {B.3}{\ignorespaces Evaluation of text segmentation using a Sliding Window with window size 3, text chunks derived from the entry "Henry Dunant award Nobel Peace Prize".}}{104}{table.caption.92}%

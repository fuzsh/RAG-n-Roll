\babel@toc {english}{}\relax 
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {3.1}{\ignorespaces Examples of Human-Understandable Text Generation, illustrates entries from multiple knowledge graphs, detailing the transformation of raw triples into readable sentences.}}{19}{table.caption.26}%
\contentsline {table}{\numberline {3.2}{\ignorespaces Example of generated questions by the Gemma2 model with relevance scores assigned by the Jina Re-ranker Cross-Encoder.}}{21}{table.caption.29}%
\contentsline {table}{\numberline {3.3}{\ignorespaces Performance of our LLM-based tasks in production, generated by Openlit~\let \reserved@d =[\def \par }}{34}{table.caption.33}%
\contentsline {table}{\numberline {3.4}{\ignorespaces Performance of our information retrieval mechanisms.}}{34}{table.caption.35}%
\contentsline {table}{\numberline {3.5}{\ignorespaces Comprenhensive list of decision points in the pipeline flow.}}{35}{table.caption.37}%
\contentsline {table}{\numberline {3.6}{\ignorespaces Limitations of the pipeline, categorized by scope of knowledge.}}{37}{table.caption.38}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {4.1}{\ignorespaces Statistical summary of FactBench, YAGO, and DBpedia datasets}}{42}{table.caption.39}%
\contentsline {table}{\numberline {4.2}{\ignorespaces Summary of key strengths of selected candidate LLMs for knowledge graph fact verification.}}{45}{table.caption.45}%
\contentsline {table}{\numberline {4.3}{\ignorespaces System configurations for empirical evaluation}}{48}{table.4.3}%
\contentsline {table}{\numberline {4.4}{\ignorespaces Empirical evaluation results of the proposed system and candidate LLMs over the FactBench, YAGO, and DBpedia.}}{50}{table.caption.46}%
\contentsline {table}{\numberline {4.5}{\ignorespaces Statistical analysis of output tokens and request times per query across FactBench, YAGO, and DBpedia datasets for each used model.}}{50}{table.caption.48}%
\contentsline {table}{\numberline {4.6}{\ignorespaces Statistical analysis of request time per query across FactBench, YAGO, and DBpedia datasets.}}{51}{table.caption.49}%
\contentsline {table}{\numberline {4.7}{\ignorespaces Partition-wise evaluation results of the proposed system and candidate LLMs over the DBpedia dataset.}}{52}{table.caption.51}%
\contentsline {table}{\numberline {4.8}{\ignorespaces Dataset-wise error clustering based on LLM-generated reasoning, using Contextual Document Embeddings for embeddings, UMAP, and HDBSCAN.}}{55}{table.caption.54}%
\addvspace {10\p@ }
\contentsline {table}{\numberline {5.1}{\ignorespaces Performance comparison of various distilled MS MARCO models based on BERT architecture, measured across NDCG@10 on TREC DL 2019 and MRR@10 on MS MARCO Dev benchmarks.}}{66}{table.caption.61}%
\contentsline {table}{\numberline {5.2}{\ignorespaces Performance evaluation of various document retrieval methods on the FactBench dataset, using the Gemma2 model.}}{68}{table.caption.63}%
\contentsline {table}{\numberline {5.3}{\ignorespaces Comparison of characteristics of embedding models}}{74}{table.caption.69}%
\contentsline {table}{\numberline {5.4}{\ignorespaces Performance evaluation of various embedding models on the FactBench dataset, using the Gemma2 model.}}{75}{table.caption.70}%
\contentsline {table}{\numberline {5.5}{\ignorespaces Advantages and Limitations of different chunking strategies for RAG systems.}}{79}{table.caption.73}%
\contentsline {table}{\numberline {5.6}{\ignorespaces Performance evaluation of various chunking strategy on the FactBench dataset, using the Gemma2 model.}}{80}{table.caption.74}%
\contentsline {table}{\numberline {5.7}{\ignorespaces Performance evaluation of similarity cut-off method on the FactBench dataset, using the Gemma2 model.}}{81}{table.caption.75}%
\contentsline {table}{\numberline {5.8}{\ignorespaces Performance evaluation of different Top\_k retrieval strategies on the FactBench dataset using the Gemma2 model. }}{81}{table.caption.77}%
\contentsline {table}{\numberline {5.9}{\ignorespaces Category-wise performance evaluation results of various models on the FactBench dataset.}}{83}{table.caption.80}%
\contentsline {table}{\numberline {5.10}{\ignorespaces Performance evaluation of various models on the FactBench dataset.}}{83}{table.caption.81}%
\contentsline {table}{\numberline {5.11}{\ignorespaces Example of failure cases and error analysis observed in the FactBench dataset using generated results and explanations.}}{84}{table.caption.83}%
\addvspace {10\p@ }
\addvspace {10\p@ }
\addvspace {10\p@ }
\contentsline {table}{\numberline {B.1}{\ignorespaces Evaluation of text segmentation using a chunk Size of 512, text chunks derived from the entry "Henry Dunant award Nobel Peace Prize".}}{94}{table.caption.86}%
\contentsline {table}{\numberline {B.2}{\ignorespaces Evaluation of text segmentation using a Small to Big technique (base chunk size 1024), text chunks derived from the entry "Henry Dunant award Nobel Peace Prize".}}{95}{table.caption.87}%
\contentsline {table}{\numberline {B.3}{\ignorespaces Evaluation of text segmentation using a Sliding Window with window size 3, text chunks derived from the entry "Henry Dunant award Nobel Peace Prize".}}{96}{table.caption.88}%

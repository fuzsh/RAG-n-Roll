\chapter{Empirical Evaluation}\label{ch:empirical-evaluation}

\section{Dataset Analysis}\label{sec:empirical-evaluation:dataset-analysis}
This section presents an analysis of the three datasets used in our empirical evaluation: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.
Each dataset offers unique characteristics and challenges, providing a comprehensive basis for assessing our knowledge graph fact verification system.
\subsection{FactBench Dataset}\label{subsec:empirical-evaluation:dataset-analysis:factbench}
\textit{FactBench} is a multilingual dataset specifically designed for fact-checking in knowledge graphs~\footnote{\url{https://github.com/DeFacto/FactBench}}~\cite{GERBER201585}.
It comprises 2,800 facts, 1.500 true and 1.300 false, across three languages: English, German, and French.
The dataset covers various domains, including geography, politics, and entertainment.
The data was automatically extracted from Wikipedia\footnote{\url{https://www.wikipedia.org/}} (DBpedia respectively) and Freebase\footnote{\url{https://developers.google.com/freebase}}.

To obtain positive examples, the authors leverages facts from both DBpedia and Freebase.
For each property under consideration, they generated these examples by issuing either a SPARQL (for DBpedia) or MQL (for Freebase) query.
They then selected the top 150 results.
In Freebase, results are ranked using an internal relevance score, while in DBpedia, the results are sorted by the number of inbound links to the resource’s corresponding Wikipedia page.
In total, 1500 correct statements were collected, with 750 allocated to both the test and training sets, ensuring that each relation had 150 positive facts equally distributed between the test and training sets.

For generating incorrect facts (negative examples), the authors modified correct ones while adhering to domain and range constraints.
To ensure that the negative examples closely resemble true statements (\ie, meaningful triples), the team altered the positive examples while still adhering to domain and range restrictions.
Given a triple (s, p, o) and its timespan (from, to) from the knowledge base, they used different methods to generate sets of negative examples.
These methods include modifying the subject, object, both subject and object, or the property.
Additionally, they included random modifications, a 20\% mix of these methods, and variations in the date.

We don't consider the time aspect in our evaluation, as our system is not designed to handle time-sensitive issues.
We consider a configuration where incorrect facts are a mix produced using different negative example generation strategies, resulting in a ground-truth accuracy of $\mu$ = 0.54.
Key characteristics of \textit{FactBench} include:
\begin{itemize}
    \item Multilingual support (English, German, and French)
    \item Diverse fact types, including domain-specific and temporal facts
    \item Manually curated for high-quality ground truth
\end{itemize}

In our analysis, we found that \textit{FactBench} presents a balanced challenge for our system, with a mix of straightforward and complex fact verification tasks.
\subsection{YAGO Dataset}\label{subsec:empirical-evaluation:dataset-analysis:yago}
YAGO (Yet Another Great Ontology) is a large-scale knowledge base derived from Wikipedia, WordNet~\footnote{\url{https://wordnet.princeton.edu/}}, and GeoNames~\footnote{\url{https://www.geonames.org/}}.
For our evaluation, we use \textit{YAGO2-sample}~\footnote{\url{https://aclanthology.org/attachments/D17-1183.Attachment.zip}}~\cite{ojha-talukdar-2017-kgeval}, a subset of the full YAGO2 knowledge graph derived from AMIE horn clauses~\cite{Yago_AMIE}.
This sample consists of 1,386 beliefs spanning 16 unique predicates.

Key characteristics of the \textit{YAGO} dataset in our evaluation include:
\begin{itemize}
    \item High accuracy: The gold standard accuracy of the \textit{YAGO2-sample} is 99.20\%, indicating a very high-quality dataset.
    \item Diverse predicates: The sample covers 16 different predicates, allowing for evaluation across a range of relationship types.
    \item Balanced distribution: Unlike domain-specific datasets, \textit{YAGO2-sample} covers a broad range of topics, reflecting the diverse nature of Wikipedia.
\end{itemize}

The high accuracy of the \textit{YAGO2-sample} presents a unique challenge for our evaluation system.
\subsection{DBpedia Dataset}\label{subsec:empirical-evaluation:dataset-analysis:dbpedia}
\textit{DBpedia} serves as a comprehensive, large-scale knowledge base derived from Wikipedia, offering structured information about millions of entities.
For our evaluation, we utilize \textit{DBpedia} version 2015-10 which contains approximately 6.2M entities and 1.1B triplets.
Following Marchesin et al.'s approach~\cite{Marchesin_Silvello_Alonso_2024} to entity-oriented research, several filtering criteria were applied to ensure high-quality data for evaluation.

The analysis was restricted to subject entities that include both: 1) rdfs:label predicate and 2) rdfs:comment predicate

Additionally, they focused exclusively on A-Box triplets (assertional knowledge) while excluding T-Box triplets (terminological knowledge).
The T-Box encompasses ontological entities and relationships, while A-Box contains the actual assertions that need verification.
After applying these filters, their working dataset consisted 4.6M entities with 170M triplets.

From this filtered dataset, they conducted a comprehensive annotation study on 9,930 facts, which were carefully selected to represent diverse types of relationships and knowledge domains within DBpedia.
To ensure annotation quality, Marchesin et al. implemented several measures:
\begin{itemize}
    \item Multiple annotators per fact (minimum of three annotations per triplet)
    \item Expert consensus requirement for final labels
    \item Binary validation approach treating all incorrect facts equally regardless of error type
    \item Documented agreement rates between expert annotators (77\% agreement with Cohen's κ score of 0.51)
    \item Third-party resolution for 82\% of initial disagreements
\end{itemize}

The dataset was carefully curated to ensure a manageable yet representative evaluation set, derived from the vast scale of \textit{DBpedia}.
By utilizing this curated subset of \textit{DBpedia}, we benefit from a balance between the richness of a real-world knowledge graph and the practicality required for thorough empirical evaluation.
For our evaluation system, we use subset of this annotated dataset, by removing facts with \textit{<UNK>} labels, resulting in 9,344 facts.

\subsubsection{Dataset Summary}\label{subsubsec:empirical-evaluation:dataset-analysis:dbpedia:summary}
To conclude, our empirical evaluation utilizes three distinct datasets: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.
Each dataset offers unique characteristics that allow us to assess our knowledge graph veracity framework across diverse scenarios.
Table~\ref{tab:dataset-summary} summarizes the key features of these datasets:

\begin{table}[h!]
    \centering
    \caption{Statistical summary of FactBench, YAGO, and DBpedia datasets}
    \begin{tabular}{lccc}
        \toprule
        & \textbf{FactBench} & \textbf{YAGO} & \textbf{DBpedia} \\
        \midrule
        Num. of Facts & 2,800 & 1,386 & 9,344 \\
        Num. of Predicates & 10 & 16 & 1,092 \\
%        Query-Entity Pairs & N/A & N/A & N/A \\
        Avg. Facts per Entity & 2.42 & 1.69 & 3.18 \\
        Gold Accuracy ($\mu$) & 0.54 & 0.99 & 0.85 \\
        \bottomrule
    \end{tabular}
    \label{tab:dataset-summary}
\end{table}

\textit{FactBench} provides a good distribution of true and false statements across multiple domains, offering a robust testbed for fact verification.
\textit{YAGO}, with its high accuracy, challenges our framework to detect subtle inaccuracies in an otherwise highly reliable knowledge graph.
The \textit{DBpedia} subset, curated specifically for entity-oriented search tasks, allows us to evaluate our framework in the context of query-dependent fact checking.
This diverse selection of datasets enables a comprehensive evaluation of our veracity estimation framework.

\section{Candidate Models}\label{sec:empirical-evaluation:candidate-models}
\subsection{Gemma2}\label{subsec:empirical-evaluation:candidate-models:gemma2}
Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models~\footnote{\url{https://deepmind.google/technologies/gemini/#introduction}}.
They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants.
Gemma 2 implements a similar architecture to the original Gemma model, with a few key differences.
The model alternates between local sliding window attention with a 4096-token span and global attention with an 8192-token span in alternate layers.
Logits are capped within a specified range to stabilize the values during attention and final layers, with soft\_cap set to 50 for self-attention layers and 30 for the final layer.
RMSNorm is used for normalization in transformer sub-layers, and \ac{GQA} with two groups enhances inference speed without sacrificing performance.
This hybrid approach aims to balance efficiency with the ability to capture long-range dependencies in the input.

We selected the \textit{Gemma2-9B} model for our evaluation, which has 9 billion parameters.
The 9B model learns from a larger teacher model during initial training in pre-training and use on-policy distillation to refine its performance post-training.
This approach allows \textit{Gemma2-9B} to capture the knowledge and capabilities of the larger model while maintaining a more compact size.
As a result, \textit{Gemma2-9B} delivers competitive performance relative to models 2-3 times its size, making it an attractive choice for applications with computational constraints.

\subsection{Qwen2.5}\label{subsec:empirical-evaluation:candidate-models:qwen2.5}
\textit{Qwen2.5} is the latest series of Qwen LLMs~\cite{qwen2}.
For \textit{Qwen2.5}, Alibaba Cloud~\footnote{\url{https://www.alibabacloud.com/en?_p_lc=7}} release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.
All models are pre-trained on our latest large-scale dataset, encompassing up to 18 trillion tokens.
Compared to \textit{Qwen2}, \textit{Qwen2.5} has acquired significantly more knowledge and has greatly improved capabilities in coding and mathematics.
Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (\eg, tables), and generating structured outputs especially JSON.
\textit{Qwen2.5} models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.
Like \textit{Qwen2}, the \textit{Qwen2.5} language models support up to 128K tokens and can generate up to 8K tokens.
They also maintain multilingual support for over 29 languages~\cite{qwen2.5}.

We selected the \textit{Qwen2.5-7b} model for our evaluation, which has 7 billion parameters.

\subsection{Llama3.1}\label{subsec:empirical-evaluation:candidate-models:llama3.1}
The Meta \textit{Llama3.1} collection of multilingual LLMs is a collection of pre-trained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).
The \textit{Llama3.1} instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.

\textit{Llama3.1} is an auto-regressive language model that uses an optimized transformer architecture.
\textit{Llama3.1} was pre-trained on ~15 trillion tokens of data.
In post-training The models produced by doing several rounds of alignment on top of the pre-trained model.
Each round involves \ac{SFT}, \ac{RS}, and \ac{DPO}.
Meta use synthetic data generation to produce the vast majority of our SFT examples, iterating multiple times to produce higher and higher quality synthetic data across all capabilities.
Additionally, they invest in multiple data processing techniques to filter this synthetic data to the highest quality.
This enables model to scale the amount of fine-tuning data across capabilities.
Compared to previous versions of Llama, developers improved both the quantity and quality of the data we use for pre and post-training.
These improvements include the development of more careful pre-processing and curation pipelines for pre-training data, the development of more rigorous quality assurance, and filtering approaches for post-training data~\cite{dubey2024llama3herdmodels,meta2023llama3}.

We selected the \textit{Llama3.1-8b} model for our evaluation, which has 8 billion parameters.

\subsection{Mistral}\label{subsec:empirical-evaluation:candidate-models:mistral}
The \textit{Mistral} model, released by Mistral AI~\footnote{\url{https://mistral.ai/}}, is a high-performance LLM, designed to outperform larger models in efficiency and effectiveness.
With innovations such as \ac{GQA} and \ac{SWA}, Mistral offers faster inference and better handling of long sequences, reducing computation costs while maintaining high performance~\cite{jiang2023mistral7b,mistral7b_2023}.

We selected the \textit{Mistral-7b} model for our evaluation, which has 7.3 billion parameters, its structure allows it to be both cost-effective and memory efficient, making it suitable for a wide variety of real-world applications

\subsubsection{Candidate Model Summary}\label{subsubsec:empirical-evaluation:candidate-models:summary}
The selection of candidate models for our system was guided by the need for diversity, efficiency, and reliability in processing fact verification tasks within knowledge graphs.
We chose \textit{Gemma2}, \textit{Qwen2.5}, \textit{Llama3.1}, and \textit{Mistral} for their specific strengths in handling diverse linguistic queries, reasoning capabilities, and compatibility with RAG pipelines.
Each of these models brings unique advantages to our verification framework, as summarized in Table~\ref{tab:candidate_models}.

\begin{table}[h!]
    \footnotesize
    \caption{Summary of key strengths of selected candidate LLMs for knowledge graph fact verification.}
    \begin{xltabular}{\linewidth}{cp{3.6cm}X}
        \toprule
        \textbf{Model} & \textbf{Key Strengths} & \textbf{Description} \\
        \midrule
        \multirow{3}{*}{Gemma2} & \multirow{3}{*}{\shortstack{Dense Retrieval \\\& Query Processing}} & Optimized for dense retrieval tasks, Gemma2 processes complex linguistic structures, making it well-suited for entity-rich query generation and document ranking. \\
        \hline
        \multirow{4}{*}{Qwen2.5} & \multirow{4}{*}{\shortstack{Logical Reasoning \\\& Prompt Efficiency}} & Excels in reasoning tasks with minimal prompting. Its accuracy in logical inference supports consistent veracity assessments, especially for ambiguous or conflicting evidence. \\
        \hline
        \multirow{5}{*}{Llama3.1} & \multirow{5}{*}{Efficiency \& Versatility} & Offers a balance of efficiency and accuracy, with robust performance across fact-checking benchmarks. Llama3.1's lower computational demands ensure responsive processing without compromising output quality. \\
        \hline
        \multirow{4}{*}{Mistral} & \multirow{4}{*}{\shortstack{Context Sensitivity \\\& Interpretability}} & Known for nuanced, context-driven outputs and interpretability. Mistral’s language generation capabilities provide clear, human-like explanations, making it ideal for understanding the facts like a human. \\
        \bottomrule
    \end{xltabular}
    \label{tab:candidate_models}
\end{table}

For model selection, we can choose either instruction-tuned and quantized models with similar architectures or with different architectures.
Here, we opted for models with varied architectures to make the ensemble more versatile and capable of handling a wide range of query scenarios.
Using diverse models in the ensemble offers a balanced approach to complex fact verification tasks across knowledge graphs.
This multi-model setup enhances adaptability and reliability, allowing the system to respond accurately to diverse verification scenarios, even when they differ in nature.

\section{Experimental Setup}\label{sec:empirical-evaluation:experimental-setup}
\subsection{Performance Metrics and Evaluation}\label{subsec:empirical-evaluation:experimental-setup:performance-metrics-and-evaluation}
Performance metrics are essential in assessing the efficacy, efficiency, and reliability of a system or model.
The selection of metrics mostly depends on the characteristics of the task, the data, and the objectives.
This section emphasizes the principal performance metrics typically employed in systems utilizing LLMs, information retrieval, and various machine learning tasks.
\subsubsection{Correct and Incorrect Criteria}
The system incorporates explicit CORRECT and INCORRECT states, indicating a binary evaluation mechanism for overall performance.
This fundamental assessment provides a clear, high-level indication of the system's success in handling queries.
\subsubsection{Relevance and Accuracy Metrics}
The evaluation of a fact-checking system typically involves assessing both the correctness and relevance of responses.

Potential metrics include:
\begin{itemize}
%    \item \textbf{Recall:} The proportion of relevant responses generated by the system among all possible relevant responses.
%    \item \textbf{Precision:} The proportion of correct responses among all generated responses.
    \item \textbf{F1 Score:} The harmonic mean of precision and recall, providing a balanced measure of accuracy.
    \item \textbf{Accuracy:} The proportion of correct responses generated by the system.
\end{itemize}
\subsubsection{Latency and Efficiency Measures}
Given the complexity of the pipeline, evaluating its operational efficiency is crucial:
\begin{itemize}
    \item \textbf{Response Time:} Measuring the end-to-end time from query input to response generation.
    \item \textbf{Component-wise Latency:} Assessing the processing time of individual pipeline components (\eg, embedding generation, LLM processing). Fully reported in ablation study in chapter~\ref{ch:ablation} and performance report in section~\ref{sec:performance-report}.
%    \item \textbf{Resource Utilization:} Monitoring computational resource usage, particularly important given the use of multiple LLMs.
    \item \textbf{Cost Efficiency:} Evaluating the cost-effectiveness of the pipeline in terms of computational resources and infrastructure by reporting the average token used per query.
\end{itemize}
\subsubsection{Consistency Evaluation}
The use of multiple models and a conflict resolution mechanism necessitates specific evaluation of output consistency:

\begin{itemize}
    \item \textbf{Stability Across Models:} Assessing the consistency of responses generated by different LLMs for the same query, refer to Algorithm~\ref{alg:stability-across-queries}.
\end{itemize}
\begin{algorithm}
    \caption{Calculate Model Consistency Per Model}
    \begin{algorithmic}[1]
        \Procedure{ModelStabilityCal}{$models$} \Comment{Containing binary results}
            \State $m\_len \gets \text{length}(models)$, $stabilityScores \gets []$

            \For{$i \gets 0$ to $m\_len - 1$}
                \State $m1 \gets models[i]$, $mStabilities \gets []$

                \For{$j \gets 0$ to $m\_len - 1$}
                    \If{$i \neq j$}
                        \State $m2 \gets models[j]$, $matchCount \gets 0$
                        \State $totPreds \gets \text{length}(m1)$

                        \For{$k \gets 0$ to $totalPredictions - 1$}
                            \If{$m1[k] = m2[k]$}
                                \State $matchCount \gets matchCount + 1$
                            \EndIf
                        \EndFor

                        \State $mStabilities$ \gets $\dfrac{matchCount}{totPreds}$ \Comment{Append the stability score}
                    \EndIf
                \EndFor

                \State $stabilityScores[i] \gets \text{mean}(mStabilities)$
            \EndFor

            \State \Return $stabilityScores$ \Comment{Dictionary with model stability scores}
        \EndProcedure
    \end{algorithmic}
    \label{alg:stability-across-queries}
\end{algorithm}

\subsection{System Configurations}\label{subsec:empirical-evaluation:experimental-setup:system-configurations}
The system configurations are selected based on the best results obtained from black-box testing the pipeline through a series of experiments, detailed in chapter~\ref{ch:ablation}.
Table~\ref{tab:system-configurations} summarizes the key system configurations used in our empirical evaluation.

{
    \noindent
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{lp{2.9cm}X}
        \caption{System configurations for empirical evaluation} \\
        \toprule
        \textbf{Section} & \textbf{Parameter} & \textbf{Considerations} \\
        \midrule
        \multirow{4}{*}{Human Understanble Text} & \multirow{4}{*}{Gemma2:9b} & Other LLMs can be used, but using instruction-tuned models is recommended. This is skipped for \textit{FactBench} dataset as discussed on~\ref{subsec:human-understandable-text-generation}. \\
        \hline
        \multirow{3}{*}{Question Generation} & \multirow{3}{*}{Gemma2:9b} & Other LLMs can be used, but using instruction-tuned models is recommended. \\
        \hline
        \multirow{2}{*}{Question Relevance} & Jina-reranker-v1-turbo-en & Cross-encoder models are recommended for this task. \\
        \hline
        Question RelevanceThreshold  & 0.5 & -- \\
        \hline
        Num. of Selected Questions & 3 & -- \\
        \hline
        \multirow{6}{*}{Google Search} & \multirow{6}{*}{--} & Used query params: \textit{lr} = 'lang\_en', \textit{gl} = 'us', \textit{hl} = 'en', \textit{num} = '100'. The lr parameter is set to the language of the query, gl to the country, hl to the language, and num to the number of results. \\
        \hline
        Num. of Selected Documents & 10 & -- \\
        \hline
        \multirow{8}{*}{Document Selection} & \multirow{8}{*}{\shortstack{ms-marco-MiniLM\\-L-6-v2}} & Filtered out the documents from these origins: dbpedia, wikipedia, wikimedia, wikidata, quora, britannica, scholarpedia, newworldencyclopedia, everipedia, encyclopedia, wikibooks, wiktionary, wikiversity, wikisource, wikiquote, wikivoyage, academia, and nytimes \\
        \hline
        Embedding Model & bge-small-en-v1.5 & -- \\
        \hline
        \multirow{2}{*}{Chunking Strategy} & Sliding Window window size 3 & -- \\
        \hline
        Similarity Cut-off & Simple & Use the threshold to filter out irrelevant documents. \\
        \hline
        Similarity Cut-off Threshold & 0.3 & -- \\
        \hline
        Top\_k & 6 & -- \\
        \hline
        \multirow{4}{*}{Tie-Breaking} & \multirow{4}{*}{--} & Use model with higher-param for each model, for llama3.1:8b~$\rightarrow$~70b, gemma2:9b~$\rightarrow$~27b, qwen2.5:7b~$\rightarrow$~14b, and mistral:7b~$\rightarrow$~mistral nemo:12b. \\
        \bottomrule
        \label{tab:system-configurations}
    \end{tabularx}
}

The tests are run on a server with the following specifications:
\begin{itemize}
    \item \textbf{Model Name:} Mac Studio
    \item \textbf{Model Identifier:} Mac14,14
    \item \textbf{Model Number:} Z180000M3T/A
    \item \textbf{Chip:} Apple M2 Ultra
    \item \textbf{Total Number of Cores:} 24 (16 performance and 8 efficiency)
    \item \textbf{Memory:} 192 GB
    \item \textbf{System Firmware Version:} 11881.1.1
    \item \textbf{OS Loader Version:} 11881.1.1
\end{itemize}
%The timing tests are conducted over 50 samples for each section, using a MacBook Pro equipped with an Apple M2 Max chip and 32 GB of memory.

\section{Comparative Analysis}\label{sec:empirical-evaluation:comparative-analysis}
\begin{table}[ht!]
    \noindent
    \caption{Empirical evaluation results of the proposed system and candidate models over the datasets.}
    {\scriptsize ms-marco-MiniLM-L-6-v2, BAAI/bge-small-en-v1.5, Sliding Window (ws 3), Similarity Cut-off (Original), Top\_k 6}
    \resizebox{\textwidth}{!}{
        \begin{threeparttable}
        \begin{tabular}{llccc||cc}
            \toprule
            \textbf{Dataset}            & \textbf{Model}                     & \textbf{Consistency}\tnote{a}  & \textbf{Avg. Request Time}\tnote{b} & \textbf{Avg. tokens per request} & \textbf{ACC} & \textbf{F1} \\
            \midrule
            \multirow{6}{*}{FactBench}  & Gemma2                             & 0.8720                         & 5.6826s                            & 1605.29                                       & \textbf{0.9032}    & \textbf{0.9101}   \\
                                        & Qwen2.5                            & 0.8685                         & 6.3094s                            & 1652.66                                       & 0.8729    & 0.8888   \\
                                        & LLama3.1                           & 0.8291                         & 6.5270s                            & 1679.04                                       & 0.8154    & 0.8299   \\
                                        & Mistral                            & 0.8650                         & 4.6692s                            & 1594.76                                       & 0.8511    & 0.8733   \\ \cline{2-7}
                                        & Proposed (At\_Most)                & \textbf{0.9176}                & 16.815s                            & 1604.196                                       & 0.8964    & 0.9071   \\
                                        & Proposed (At\_Least)               & N/A                            & N/A                            & N/A                                       & N/A    & N/A   \\ \hline \hline
            \multirow{6}{*}{YAGO}       & Gemma2                             & 0.8785                         & 5.1982s                            & 1597.50                                       & 0.8499    & 0.9187   \\
                                        & Qwen2.5                            & 0.8773                         & 6.2770s                            & 1653.56                                       & 0.8506    & 0.9191   \\
                                        & LLama3.1                           & 0.8276                         & 6.4070s                            & 1682.78                                       & 0.8059    & 0.8922   \\
                                        & Mistral                            & 0.8817                         & 4.4824s                            & 1589.58                                       & \textbf{0.9250}    & \textbf{0.9610}   \\ \cline{2-7}
                                        & Proposed (At\_Most)                & \textbf{0.9226}                & 6.4810s                            & 1586.02                                       & 0.8737    & 0.9325   \\
                                        & Proposed (At\_Least)               & N/A                            & N/A                            & N/A                                       & N/A    & N/A   \\ \hline \hline
            \multirow{6}{*}{DBpedia}    & Gemma2                             & N/A                          & N/A                            & N/A                                       & \textbf{N/A}    & \textbf{N/A }   \\
                                        & Qwen2.5                            & N/A                          & N/A                            & N/A                                       & N/A     & N/A    \\
                                        & LLama3.1                           & N/A                          & N/A                            & N/A                                       & N/A     & N/A    \\
                                        & Mistral                            & N/A                          & N/A                            & N/A                                       & N/A     & N/A    \\ \cline{2-7}
                                        & Proposed (At\_Most)                & \textbf{N/A }                & N/A                            & N/A                                       & N/A     & N/A    \\
                                        & Proposed (At\_Least)               & \textbf{N/A }                & N/A                            & N/A                                       & N/A     & N/A    \\

            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \item[a] Consistency score for each individual model is calculated across all models, excluding the ensemble models.
            \item[b] Each query uses two requests, so the average request duration is calculated based on the two requests.
        \end{tablenotes}
        \end{threeparttable}}
    \label{tab:evaluation_results-full-wo-category-all-datasets}
\end{table}


\subsection{Discussion of Results}\label{subsec:discussion-of-results}
The empirical evaluation results presented in Table~\ref{tab:evaluation_results-full-wo-category-all-datasets} offer valuable insights into the performance of our proposed system and the individual candidate models across the three datasets: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.

Among the candidate models, \textit{Llama3.1} generates the reasoning most of the time and don't follow the instructions fully, this way the Avg completion tokens is higher than the other models.

In terms of overall accuracy and F1 scores, the \textit{Gemma2} model outperforms the other individual models on the FactBench dataset.
This superior performance can be attributed to Gemma2's advanced attention mechanisms, which employ a hybrid approach of sliding window attention and global attention to effectively capture both local and long-range dependencies in the input sequences.
The model's ability to learn from larger teacher models during pre-training and its use of on-policy distillation post-training likely contribute to its strong performance relative to its size.

Interestingly, the \textit{Gemma2} model achieves the highest accuracy and F1 scores on the YAGO dataset, surpassing even \textit{Gemma2}.
This may be due to Mistral's optimized transformer architecture, which incorporates innovations such as Grouped-Query Attention and Sliding Window Attention to improve inference speed and handle longer sequences more effectively.

The \textit{Qwen2.5} and \textit{LLama3.1} models generally fall in the middle of the pack in terms of accuracy and F1 scores.
While both models leverage large-scale pre-training and instruction tuning, their performance suggests room for improvement in the specific task of knowledge graph fact verification.

Notably, our proposed system, which employs an ensemble approach using the combination strategy, consistently achieves competitive accuracy and F1 scores compared to the best-performing individual models.
This highlights the effectiveness of combining the strengths of multiple models through majority voting and adaptive dispute resolution techniques.
However, the increased computational cost of running multiple models in parallel is reflected in the higher average request duration for the proposed system.
The stability scores provide another dimension for comparison, indicating the consistency of model outputs across different runs.
The proposed system achieves the highest stability scores on datasets, showing the robustness gained by synthesizing results from multiple models.

%TODO: CHECK WHEN DBPEDIA COMPLETED
Among the individual models, \textit{Gemma2} and \textit{Mistral} exhibit relatively high stability, while \textit{LLama3.1} has the lowest stability scores.
\subsection{Key Findings}\label{subsec:empirical-evaluation:discussion-of-results:key-findings}
\subsection{Error Analysis}\label{subsec:empirical-evaluation:discussion-of-results:error-analysis}

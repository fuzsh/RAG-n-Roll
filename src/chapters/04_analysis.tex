\chapter{Empirical Evaluation}\label{ch:empirical-evaluation}

\section{Experimental Setup}\label{sec:empirical-evaluation:experimental-setup}
\subsection{Datasets}\label{subsec:empirical-evaluation:experimental-setup:datasets}
\subsection{System Configurations}\label{subsec:empirical-evaluation:experimental-setup:system-configurations}
\subsection{Performance Metrics and Evaluation}\label{subsec:empirical-evaluation:experimental-setup:performance-metrics-and-evaluation}
Performance metrics are essential in assessing the efficacy, efficiency, and reliability of a system or model.
The selection of metrics mostly depends on the characteristics of the task, the data, and the objectives.
This section emphasizes the principal performance metrics typically employed in systems utilizing LLMs, information retrieval, and various machine learning tasks.
\subsubsection{Correct and Incorrect Criteria}
The system incorporates explicit CORRECT and INCORRECT states, indicating a binary evaluation mechanism for overall performance.
This fundamental assessment provides a clear, high-level indication of the system's success in handling queries.
\subsubsection{Relevance and Accuracy Metrics}
The evaluation of a fact-checking system typically involves assessing both the correctness and relevance of responses.

Potential metrics include:
\begin{itemize}
    \item \textbf{Recall:} The proportion of relevant responses generated by the system among all possible relevant responses.
    \item \textbf{Precision:} The proportion of correct responses among all generated responses.
    \item \textbf{F1 Score:} The harmonic mean of precision and recall, providing a balanced measure of accuracy.
    \item \textbf{Accuracy:} The proportion of correct responses generated by the system.
\end{itemize}
\subsubsection{Latency and Efficiency Measures}
Given the complexity of the pipeline, evaluating its operational efficiency is crucial:
\begin{itemize}
    \item \textbf{Response Time:} Measuring the end-to-end time from query input to response generation.
    \item \textbf{Component-wise Latency:} Assessing the processing time of individual pipeline components (e.g., embedding generation, LLM processing).
    \item \textbf{Resource Utilization:} Monitoring computational resource usage, particularly important given the use of multiple LLMs.
    \item \textbf{Cost Efficiency:} Evaluating the cost-effectiveness of the pipeline in terms of computational resources and infrastructure.
\end{itemize}
\subsubsection{Consistency Evaluation}
The use of multiple models and a conflict resolution mechanism necessitates specific evaluation of output consistency:

\begin{itemize}
    \item \textbf{Stability Across Models:} Assessing the consistency of responses generated by different LLMs for the same query, refer to algorithm~\ref{alg:stability-across-queries}.
\end{itemize}
\begin{algorithm}
    \caption{Stability Across Queries}
    \begin{algorithmic}[1]
        \Procedure{StabilityAcrossQueries}{$queryResponses$}
            \State $stabilityScores \gets$ empty list
            \For{each $responses$ in $queryResponses$}
                \State $uniqueResponses \gets$ set of unique elements in $responses$
                \State $stability \gets$ size of $uniqueResponses$
                \State $stabilityScore \gets (1 - \frac{stability - 1}{|responses|}) \times 100$
                \State Append $stabilityScore$ to $stabilityScores$
                \If{$stabilityScores$ is not empty}
                    \State \Return mean of $stabilityScores$
                \Else
                    \State \Return 0
                \EndIf
            \EndFor
        \EndProcedure
    \end{algorithmic}\label{alg:stability-across-queries}
\end{algorithm}
\section{Dataset Analysis}\label{sec:empirical-evaluation:dataset-analysis}
This section presents an analysis of the three datasets used in our empirical evaluation: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.
Each dataset offers unique characteristics and challenges, providing a comprehensive basis for assessing our knowledge graph fact verification system.
\subsection{FactBench Dataset}\label{subsec:empirical-evaluation:dataset-analysis:factbench}
\textit{FactBench} is a multilingual dataset specifically designed for fact-checking in knowledge graphs~\footnote{\url{https://github.com/DeFacto/FactBench}}~\cite{GERBER201585}.
It comprises 2,800 facts, 1.500 true and 1.300 false, across three languages: English, German, and French.
The dataset covers various domains, including geography, politics, and entertainment.
The data was automatically extracted from Wikipedia\footnote{\url{https://www.wikipedia.org/}} (DBpedia respectively) and Freebase\footnote{\url{https://developers.google.com/freebase}}.

To obtain positive examples, the repository leverages facts from both DBpedia and Freebase.
For each property under consideration, they generated these examples by issuing either a SPARQL (for DBpedia) or MQL (for Freebase) query.
They then selected the top 150 results.
In Freebase, results are ranked using an internal relevance score, while in DBpedia, the results are sorted by the number of inbound links to the resourceâ€™s corresponding Wikipedia page.
In total, 1500 correct statements were collected, with 750 allocated to both the test and training sets, ensuring that each relation had 150 positive facts equally distributed between the test and training sets.

Generating negative examples is more complex than generating positive ones.
To ensure that the negative examples closely resemble true statements (i.e., meaningful triples), the team altered the positive examples while still adhering to domain and range restrictions.
Given a triple (s, p, o) and its timespan (from, to) from the knowledge base, they used different methods to generate sets of negative examples.
These methods include modifying the subject, object, both subject and object, or the property.
Additionally, they included random modifications, a 20\% mix of these methods, and variations in the date.

We don't consider the time aspect in our evaluation, as our system is not designed to handle time-sensitive issues.

Key characteristics of \textit{FactBench} include:
\begin{itemize}
    \item Multilingual support (English, German, and French)
    \item Diverse fact types, including domain-specific and temporal facts
    \item Manually curated for high-quality ground truth
\end{itemize}

In our analysis, we found that \textit{FactBench} presents a balanced challenge for our system, with a mix of straightforward and complex fact verification tasks.
\subsection{YAGO Dataset}\label{subsec:empirical-evaluation:dataset-analysis:yago}
YAGO (Yet Another Great Ontology) is a large-scale knowledge base derived from Wikipedia, WordNet~\footnote{\url{https://wordnet.princeton.edu/}}, and GeoNames~\footnote{\url{https://www.geonames.org/}}.
For our evaluation, we use \textit{YAGO2-sample}~\footnote{\url{https://aclanthology.org/attachments/D17-1183.Attachment.zip}}~\cite{ojha-talukdar-2017-kgeval}, a subset of the full YAGO2 knowledge graph derived from AMIE horn clauses~\cite{Yago_AMIE}.
This sample consists of 1,386 beliefs spanning 16 unique predicates.

Key characteristics of the \textit{YAGO} dataset in our evaluation include:
\begin{itemize}
    \item High accuracy: The gold standard accuracy of the \textit{YAGO2-sample} is 99.20\%, indicating a very high-quality dataset.
    \item Diverse predicates: The sample covers 16 different predicates, allowing for evaluation across a range of relationship types.
    \item Balanced distribution: Unlike domain-specific datasets, \textit{YAGO2-sample} covers a broad range of topics, reflecting the diverse nature of Wikipedia.
\end{itemize}

The high accuracy of the \textit{YAGO2-sample} presents a unique challenge for our evaluation system.
\subsection{DBpedia Dataset}\label{subsec:empirical-evaluation:dataset-analysis:dbpedia}
\textit{DBpedia} serves as a comprehensive, large-scale knowledge base derived from Wikipedia, offering structured information about millions of entities.
For our evaluation, we utilize \textit{DBpedia} version 2015-10 as provided by Hasibi et al.~\cite{10.1145/3077136.3080810}.
This subset serves as the underlying knowledge graph for our experiments.
The dataset was carefully curated to ensure a manageable yet representative evaluation set, derived from the vast scale of \textit{DBpedia}.

The dataset was constructed using the following criteria:
\begin{itemize}
    \item Entity Selection: The dataset is restricted to entities with essential attributes, including a title (rdfs:label) and a short abstract (rdfs:comment). This ensures that each entity has sufficient information for meaningful evaluation.
    \item Query Diversity: The dataset leverages the DBpedia-Entity collection~\cite{10.1145/2484028.2484165}, a standard test collection for entity retrieval encompassing a wide range of query types. This includes named entity queries, list queries, natural language questions, and keyword queries, providing a comprehensive evaluation landscape.
    \item Query-Entity Pairing: For each query in the DBpedia-Entity collection, a single relevant entity was selected. This selection process employed a voting mechanism across multiple entity retrieval approaches, ensuring that the chosen entity is both relevant and readily retrievable.
    \item Fact Extraction: For each selected entity, the corresponding facts (triples) were extracted from DBpedia. These facts form the basis of our evaluation set for tasks such as fact ranking and entity summarization.
\end{itemize}

Key characteristics of this DBpedia-based evaluation set include:
\begin{itemize}
    \item Scale: The dataset comprises 100 query-entity pairs and 4,069 corresponding facts, with an average of 41 facts per query-entity pair.
    \item Diversity: The dataset captures a wide range of relationship types and entity attributes across various domains.
    \item Quality Assurance: Manual annotation has been performed to assess the relevance and correctness of facts in relation to their corresponding queries and entities. These annotations serve as the gold standard for our evaluation tasks provided by Marchesin et al.~\cite{marchesin_etal-cikm2024}.
    \item Task Suitability: The dataset is specifically designed to support various entity-oriented search tasks, including but not limited to entity summarization, fact ranking, and query-dependent fact selection.
\end{itemize}
By utilizing this curated subset of DBpedia, we benefit from a balance between the richness of a real-world knowledge graph and the practicality required for thorough empirical evaluation.

\subsubsection{Dataset Summary}\label{subsubsec:empirical-evaluation:dataset-analysis:dbpedia:summary}
To conclude, our empirical evaluation utilizes three distinct datasets: FactBench, YAGO, and DBpedia. Each dataset offers unique characteristics that allow us to assess our knowledge graph veracity framework across diverse scenarios. Table 2 summarizes the key features of these datasets:

\begin{table}[h!]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Feature} & \textbf{FactBench} & \textbf{YAGO} & \textbf{DBpedia} \\
        \midrule
        Number of Facts & 1,860 & 1,386 & 4,069 \\
        Number of Predicates & 18 & 16 & Varied \\
        Gold Accuracy & 91.34\% & 99.20\% & N/A \\
        Number of Constraints & 130 & 28 & N/A \\
        Query-Entity Pairs & N/A & N/A & 100 \\
        Avg. Facts per Entity & N/A & N/A & 41 \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Datasets Used in Empirical Evaluation}
    \label{tab:dataset-summary}
\end{table}

\textit{FactBench} provides a good distribution of true and false statements across multiple domains, offering a robust testbed for fact verification.
\textit{YAGO}, with its high accuracy, challenges our framework to detect subtle inaccuracies in an otherwise highly reliable knowledge graph.
The \textit{DBpedia} subset, curated specifically for entity-oriented search tasks, allows us to evaluate our framework in the context of query-dependent fact ranking and summarization.
This diverse selection of datasets enables a comprehensive evaluation of our veracity estimation framework.

\section{Candidate Models}\label{sec:empirical-evaluation:candidate-models}
\subsection{Gemma2}\label{subsec:empirical-evaluation:candidate-models:gemma2}
Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models~\footnote{\url{https://deepmind.google/technologies/gemini/#introduction}}.
They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants.
Gemma 2 implements a novel approach to attention mechanisms:
\begin{itemize}
    \item Every other layer uses a sliding window attention with a local context of 4096 tokens.
    \item Alternating layers employ full quadratic global attention across the entire 8192 token context.
\end{itemize}
This hybrid approach aims to balance efficiency with the ability to capture long-range dependencies in the input.

We selected the \textit{Gemma2-9B} model for our evaluation, which has 9 billion parameters.
The 9B model learns from a larger teacher model during initial training in pre-training and use on-policy distillation to refine its performance post-training.
This approach allows Gemma2-9B to capture the knowledge and capabilities of the larger model while maintaining a more compact size.
As a result, \textit{Gemma2-9B} delivers competitive performance relative to models 2-3 times its size, making it an attractive choice for applications with computational constraints.

\subsection{Qwen2.5}\label{subsec:empirical-evaluation:candidate-models:qwen2.5}
\textit{Qwen2.5} is the latest series of Qwen LLMs~\cite{qwen2}.
For \textit{Qwen2.5}, Alibaba Cloud~\footnote{\url{https://www.alibabacloud.com/en?_p_lc=7}} release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.
All models are pre-trained on our latest large-scale dataset, encompassing up to 18 trillion tokens.
Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge and has greatly improved capabilities in coding and mathematics.
Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.
\textit{Qwen2.5} models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.
Like Qwen2, the \textit{Qwen2.5} language models support up to 128K tokens and can generate up to 8K tokens.
They also maintain multilingual support for over 29 languages~\cite{qwen2.5}.

We selected the \textit{Qwen2.5-7b} model for our evaluation, which has 7 billion parameters.

\subsection{Llama3.1}\label{subsec:empirical-evaluation:candidate-models:llama3.1}
The Meta \textit{Llama3.1} collection of multilingual LLMs is a collection of pre-trained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).
The \textit{Llama3.1} instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.

\textit{Llama3.1} is an auto-regressive language model that uses an optimized transformer architecture.
The tuned versions use \ac{SFT} and \ac{RLHF} to align with human preferences for helpfulness and safety.
All model versions use \ac{GQA} for improved inference scalability.
\textit{Llama3.1} was pre-trained on ~15 trillion tokens of data from publicly available sources.
The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples~\cite{dubey2024llama3herdmodels,meta2023llama3}.

We selected the \textit{Llama3.1-8b} model for our evaluation, which has 8 billion parameters.

\subsection{Mistral}\label{subsec:empirical-evaluation:candidate-models:mistral}
The \textit{Mistral} model, released by Mistral AI~\footnote{\url{https://mistral.ai/}}, is a high-performance LLM, designed to outperform larger models in efficiency and effectiveness.
With innovations such as \ac{GQA} and \ac{SWA}, Mistral offers faster inference and better handling of long sequences, reducing computation costs while maintaining high performance~\cite{jiang2023mistral7b,mistral7b_2023}.

We selected the \textit{Mistral-7b} model for our evaluation, which has 7.3 billion parameters, its structure allows it to be both cost-effective and memory efficient, making it suitable for a wide variety of real-world applications
\section{Comparative Analysis}\label{sec:empirical-evaluation:comparative-analysis}

\section{Discussion of Results}\label{sec:empirical-evaluation:discussion-of-results}
\subsection{Key Findings}\label{subsec:empirical-evaluation:discussion-of-results:key-findings}
\subsection{Error Analysis}\label{subsec:empirical-evaluation:discussion-of-results:error-analysis}

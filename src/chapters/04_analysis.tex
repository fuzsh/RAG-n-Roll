\chapter{Empirical Evaluation}\label{ch:empirical-evaluation}

\section{Dataset Analysis}\label{sec:empirical-evaluation:dataset-analysis}
This section presents an analysis of the three datasets used in our empirical evaluation: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.
Each dataset offers unique characteristics and challenges, providing a comprehensive basis for assessing our knowledge graph fact verification system.
\subsection{FactBench Dataset}\label{subsec:empirical-evaluation:dataset-analysis:factbench}
\textit{FactBench} is a multilingual dataset specifically designed for fact-checking in knowledge graphs~\footnote{\url{https://github.com/DeFacto/FactBench}}~\cite{GERBER201585}.
It comprises 2,800 facts, 1.500 true and 1.300 false, across three languages: English, German, and French.
The dataset covers various domains, including geography, politics, and entertainment.
The data was automatically extracted from Wikipedia\footnote{\url{https://www.wikipedia.org/}} (DBpedia respectively) and Freebase\footnote{\url{https://developers.google.com/freebase}}.

To obtain positive examples, the repository leverages facts from both DBpedia and Freebase.
For each property under consideration, they generated these examples by issuing either a SPARQL (for DBpedia) or MQL (for Freebase) query.
They then selected the top 150 results.
In Freebase, results are ranked using an internal relevance score, while in DBpedia, the results are sorted by the number of inbound links to the resource’s corresponding Wikipedia page.
In total, 1500 correct statements were collected, with 750 allocated to both the test and training sets, ensuring that each relation had 150 positive facts equally distributed between the test and training sets.

Generating negative examples is more complex than generating positive ones.
To ensure that the negative examples closely resemble true statements (i.e., meaningful triples), the team altered the positive examples while still adhering to domain and range restrictions.
Given a triple (s, p, o) and its timespan (from, to) from the knowledge base, they used different methods to generate sets of negative examples.
These methods include modifying the subject, object, both subject and object, or the property.
Additionally, they included random modifications, a 20\% mix of these methods, and variations in the date.

We don't consider the time aspect in our evaluation, as our system is not designed to handle time-sensitive issues.

Key characteristics of \textit{FactBench} include:
\begin{itemize}
    \item Multilingual support (English, German, and French)
    \item Diverse fact types, including domain-specific and temporal facts
    \item Manually curated for high-quality ground truth
\end{itemize}

In our analysis, we found that \textit{FactBench} presents a balanced challenge for our system, with a mix of straightforward and complex fact verification tasks.
\subsection{YAGO Dataset}\label{subsec:empirical-evaluation:dataset-analysis:yago}
YAGO (Yet Another Great Ontology) is a large-scale knowledge base derived from Wikipedia, WordNet~\footnote{\url{https://wordnet.princeton.edu/}}, and GeoNames~\footnote{\url{https://www.geonames.org/}}.
For our evaluation, we use \textit{YAGO2-sample}~\footnote{\url{https://aclanthology.org/attachments/D17-1183.Attachment.zip}}~\cite{ojha-talukdar-2017-kgeval}, a subset of the full YAGO2 knowledge graph derived from AMIE horn clauses~\cite{Yago_AMIE}.
This sample consists of 1,386 beliefs spanning 16 unique predicates.

Key characteristics of the \textit{YAGO} dataset in our evaluation include:
\begin{itemize}
    \item High accuracy: The gold standard accuracy of the \textit{YAGO2-sample} is 99.20\%, indicating a very high-quality dataset.
    \item Diverse predicates: The sample covers 16 different predicates, allowing for evaluation across a range of relationship types.
    \item Balanced distribution: Unlike domain-specific datasets, \textit{YAGO2-sample} covers a broad range of topics, reflecting the diverse nature of Wikipedia.
\end{itemize}

The high accuracy of the \textit{YAGO2-sample} presents a unique challenge for our evaluation system.
\subsection{DBpedia Dataset}\label{subsec:empirical-evaluation:dataset-analysis:dbpedia}
\textit{DBpedia} serves as a comprehensive, large-scale knowledge base derived from Wikipedia, offering structured information about millions of entities.
For our evaluation, we utilize \textit{DBpedia} version 2015-10 which contains approximately 6.2M entities and 1.1B triplets.
Following Marchesin et al.'s approach~\cite{Marchesin_Silvello_Alonso_2024} to entity-oriented research, several filtering criteria were applied to ensure high-quality data for evaluation.

The analysis was restricted to subject entities that include both: 1) rdfs:label predicate and 2) rdfs:comment predicate

Additionally, they focused exclusively on A-Box triplets (assertional knowledge) while excluding T-Box triplets (terminological knowledge).
The T-Box encompasses ontological entities and relationships, while A-Box contains the actual assertions that need verification.
After applying these filters, their working dataset consisted 4.6M entities with 170M triplets.

From this filtered dataset, they conducted a comprehensive annotation study on 9,930 facts, which were carefully selected to represent diverse types of relationships and knowledge domains within DBpedia.
This subset serves as the underlying knowledge graph for our experiments.

To ensure annotation quality, Marchesin et al. implemented several measures:
\begin{itemize}
    \item Multiple annotators per fact (minimum of three annotations per triplet)
    \item Expert consensus requirement for final labels
    \item Binary validation approach treating all incorrect facts equally regardless of error type
    \item Documented agreement rates between expert annotators (77\% agreement with Cohen's κ score of 0.51)
    \item Third-party resolution for 82\% of initial disagreements
\end{itemize}

The dataset was carefully curated to ensure a manageable yet representative evaluation set, derived from the vast scale of \textit{DBpedia}.
By utilizing this curated subset of \textit{DBpedia}, we benefit from a balance between the richness of a real-world knowledge graph and the practicality required for thorough empirical evaluation.

\subsubsection{Dataset Summary}\label{subsubsec:empirical-evaluation:dataset-analysis:dbpedia:summary}
To conclude, our empirical evaluation utilizes three distinct datasets: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.
Each dataset offers unique characteristics that allow us to assess our knowledge graph veracity framework across diverse scenarios.
Table~\ref{tab:dataset-summary} summarizes the key features of these datasets:

\begin{table}[h!]
    \centering
    \begin{tabular}{lccc}
        \toprule
        \textbf{Feature} & \textbf{FactBench} & \textbf{YAGO} & \textbf{DBpedia} \\
        \midrule
        Number of Facts & 2,800 & 1,386 & 9,344 \\
        Number of Predicates & 10 & 16 & 1,092 \\
        Gold Accuracy & 0.54 & 0.99 & 0.85 \\
%        Query-Entity Pairs & N/A & N/A & N/A \\
        Avg. Facts per Entity & 2.42 & 1.69 & 3.18 \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Datasets Used in Empirical Evaluation}
    \label{tab:dataset-summary}
\end{table}

\textit{FactBench} provides a good distribution of true and false statements across multiple domains, offering a robust testbed for fact verification.
\textit{YAGO}, with its high accuracy, challenges our framework to detect subtle inaccuracies in an otherwise highly reliable knowledge graph.
The \textit{DBpedia} subset, curated specifically for entity-oriented search tasks, allows us to evaluate our framework in the context of query-dependent fact checking.
This diverse selection of datasets enables a comprehensive evaluation of our veracity estimation framework.

\section{Candidate Models}\label{sec:empirical-evaluation:candidate-models}
\subsection{Gemma2}\label{subsec:empirical-evaluation:candidate-models:gemma2}
Gemma is a family of lightweight, state-of-the-art open models from Google, built from the same research and technology used to create the Gemini models~\footnote{\url{https://deepmind.google/technologies/gemini/#introduction}}.
They are text-to-text, decoder-only large language models, available in English, with open weights for both pre-trained variants and instruction-tuned variants.
Gemma 2 implements a novel approach to attention mechanisms:
\begin{itemize}
    \item Every other layer uses a sliding window attention with a local context of 4096 tokens.
    \item Alternating layers employ full quadratic global attention across the entire 8192 token context.
\end{itemize}
This hybrid approach aims to balance efficiency with the ability to capture long-range dependencies in the input.

We selected the \textit{Gemma2-9B} model for our evaluation, which has 9 billion parameters.
The 9B model learns from a larger teacher model during initial training in pre-training and use on-policy distillation to refine its performance post-training.
This approach allows Gemma2-9B to capture the knowledge and capabilities of the larger model while maintaining a more compact size.
As a result, \textit{Gemma2-9B} delivers competitive performance relative to models 2-3 times its size, making it an attractive choice for applications with computational constraints.

\subsection{Qwen2.5}\label{subsec:empirical-evaluation:candidate-models:qwen2.5}
\textit{Qwen2.5} is the latest series of Qwen LLMs~\cite{qwen2}.
For \textit{Qwen2.5}, Alibaba Cloud~\footnote{\url{https://www.alibabacloud.com/en?_p_lc=7}} release a number of base language models and instruction-tuned language models ranging from 0.5 to 72 billion parameters.
All models are pre-trained on our latest large-scale dataset, encompassing up to 18 trillion tokens.
Compared to Qwen2, Qwen2.5 has acquired significantly more knowledge and has greatly improved capabilities in coding and mathematics.
Additionally, the new models achieve significant improvements in instruction following, generating long texts (over 8K tokens), understanding structured data (e.g, tables), and generating structured outputs especially JSON.
\textit{Qwen2.5} models are generally more resilient to the diversity of system prompts, enhancing role-play implementation and condition-setting for chatbots.
Like Qwen2, the \textit{Qwen2.5} language models support up to 128K tokens and can generate up to 8K tokens.
They also maintain multilingual support for over 29 languages~\cite{qwen2.5}.

We selected the \textit{Qwen2.5-7b} model for our evaluation, which has 7 billion parameters.

\subsection{Llama3.1}\label{subsec:empirical-evaluation:candidate-models:llama3.1}
The Meta \textit{Llama3.1} collection of multilingual LLMs is a collection of pre-trained and instruction tuned generative models in 8B, 70B and 405B sizes (text in/text out).
The \textit{Llama3.1} instruction tuned text only models (8B, 70B, 405B) are optimized for multilingual dialogue use cases and outperform many of the available open source and closed chat models on common industry benchmarks.

\textit{Llama3.1} is an auto-regressive language model that uses an optimized transformer architecture.
The tuned versions use \ac{SFT} and \ac{RLHF} to align with human preferences for helpfulness and safety.
All model versions use \ac{GQA} for improved inference scalability.
\textit{Llama3.1} was pre-trained on ~15 trillion tokens of data from publicly available sources.
The fine-tuning data includes publicly available instruction datasets, as well as over 25M synthetically generated examples~\cite{dubey2024llama3herdmodels,meta2023llama3}.

We selected the \textit{Llama3.1-8b} model for our evaluation, which has 8 billion parameters.

\subsection{Mistral}\label{subsec:empirical-evaluation:candidate-models:mistral}
The \textit{Mistral} model, released by Mistral AI~\footnote{\url{https://mistral.ai/}}, is a high-performance LLM, designed to outperform larger models in efficiency and effectiveness.
With innovations such as \ac{GQA} and \ac{SWA}, Mistral offers faster inference and better handling of long sequences, reducing computation costs while maintaining high performance~\cite{jiang2023mistral7b,mistral7b_2023}.

We selected the \textit{Mistral-7b} model for our evaluation, which has 7.3 billion parameters, its structure allows it to be both cost-effective and memory efficient, making it suitable for a wide variety of real-world applications

\subsubsection{Candidate Model Summary}\label{subsubsec:empirical-evaluation:candidate-models:summary}
The selection of candidate models for our system was guided by the need for diversity, efficiency, and reliability in processing fact verification tasks within knowledge graphs.
We chose \textit{Gemma2}, \textit{Qwen2.5}, \textit{Llama3.1}, and \textit{Mistral} for their specific strengths in handling diverse linguistic queries, reasoning capabilities, and compatibility with RAG pipelines.
Each of these models brings unique advantages to our verification framework, as summarized in Table~\ref{tab:candidate_models}.

\begin{table}[h!]
    \centering
    \begin{tabular}{c|p{4cm}|p{8cm}}
        \toprule
        \textbf{Model} & \textbf{Key Strengths} & \textbf{Description} \\
        \midrule
        \textbf{Gemma2} & Dense Retrieval and Query Processing & Optimized for dense retrieval tasks, Gemma2 processes complex linguistic structures, making it well-suited for entity-rich query generation and document ranking. \\
        \hline
        \textbf{Qwen2.5} & Logical Reasoning and Prompt Efficiency & Excels in reasoning tasks with minimal prompting. Its accuracy in logical inference supports consistent veracity assessments, especially for ambiguous or conflicting evidence. \\
        \hline
        \textbf{Llama3.1} & Efficiency and Versatility & Offers a balance of efficiency and accuracy, with robust performance across fact-checking benchmarks. Llama3.1's lower computational demands ensure responsive processing without compromising output quality. \\
        \hline
        \textbf{Mistral} & Context Sensitivity and Interpretability & Known for nuanced, context-driven outputs and interpretability. Mistral’s language generation capabilities provide clear, human-like explanations, critical for user-friendly veracity decisions. \\
        \bottomrule
    \end{tabular}
    \caption{Summary of Candidate LLM Models for Knowledge Graph Fact Verification}
    \label{tab:candidate_models}
\end{table}

These models, when used in ensemble, offer a balanced approach to handling complex fact verification tasks across knowledge graphs.
The multi-model architecture ensures high adaptability and reliability, allowing the system to respond accurately to diverse verification scenarios.

\section{Experimental Setup}\label{sec:empirical-evaluation:experimental-setup}
\subsection{Performance Metrics and Evaluation}\label{subsec:empirical-evaluation:experimental-setup:performance-metrics-and-evaluation}
Performance metrics are essential in assessing the efficacy, efficiency, and reliability of a system or model.
The selection of metrics mostly depends on the characteristics of the task, the data, and the objectives.
This section emphasizes the principal performance metrics typically employed in systems utilizing LLMs, information retrieval, and various machine learning tasks.
\subsubsection{Correct and Incorrect Criteria}
The system incorporates explicit CORRECT and INCORRECT states, indicating a binary evaluation mechanism for overall performance.
This fundamental assessment provides a clear, high-level indication of the system's success in handling queries.
\subsubsection{Relevance and Accuracy Metrics}
The evaluation of a fact-checking system typically involves assessing both the correctness and relevance of responses.

Potential metrics include:
\begin{itemize}
    \item \textbf{Recall:} The proportion of relevant responses generated by the system among all possible relevant responses.
    \item \textbf{Precision:} The proportion of correct responses among all generated responses.
    \item \textbf{F1 Score:} The harmonic mean of precision and recall, providing a balanced measure of accuracy.
    \item \textbf{Accuracy:} The proportion of correct responses generated by the system.
\end{itemize}
\subsubsection{Latency and Efficiency Measures}
Given the complexity of the pipeline, evaluating its operational efficiency is crucial:
\begin{itemize}
    \item \textbf{Response Time:} Measuring the end-to-end time from query input to response generation.
    \item \textbf{Component-wise Latency:} Assessing the processing time of individual pipeline components (e.g., embedding generation, LLM processing).
    \item \textbf{Resource Utilization:} Monitoring computational resource usage, particularly important given the use of multiple LLMs.
    \item \textbf{Cost Efficiency:} Evaluating the cost-effectiveness of the pipeline in terms of computational resources and infrastructure.
\end{itemize}
\subsubsection{Consistency Evaluation}
The use of multiple models and a conflict resolution mechanism necessitates specific evaluation of output consistency:

\begin{itemize}
    \item \textbf{Stability Across Models:} Assessing the consistency of responses generated by different LLMs for the same query, refer to Algorithm~\ref{alg:stability-across-queries}.
\end{itemize}
\begin{algorithm}
    \caption{Calculate Model Stability Per Model}
    \begin{algorithmic}[1]
        \Procedure{ModelStabilityCal}{$models$} \Comment{Containing binary results}
            \State $numModels \gets \text{length}(models)$
            \State Initialize empty dictionary $stabilityScores$

            \For{$i \gets 0$ to $numModels - 1$}
                \State $model1 \gets models[i]$
                \State Initialize empty list $modelStabilities$

                \For{$j \gets 0$ to $numModels - 1$}
                    \If{$i \neq j$}
                        \State $model2 \gets models[j]$
                        \State $matchCount \gets 0$
                        \State $totalPredictions \gets \text{length}(model1)$

                        \For{$k \gets 0$ to $totalPredictions - 1$}
                            \If{$model1[k] = model2[k]$}
                                \State $matchCount \gets matchCount + 1$
                            \EndIf
                        \EndFor

                        \State Append $matchCount / totalPredictions$ to $modelStabilities$
                    \EndIf
                \EndFor

                \State $stabilityScores[i] \gets \text{mean}(modelStabilities)$
            \EndFor

            \State \Return $stabilityScores$ \Comment{Dictionary with model stability scores}
        \EndProcedure
    \end{algorithmic}\label{alg:stability-across-queries}
\end{algorithm}

\subsection{System Configurations}\label{subsec:empirical-evaluation:experimental-setup:system-configurations}
The system configurations are selected based on the best results obtained from black-box testing the pipeline through a series of experiments, detailed in section~\ref{ch:ablation}.
Table~\ref{tab:system-configurations} summarizes the key system configurations used in our empirical evaluation.

{
    \noindent
    \centering
    \footnotesize
    \begin{tabularx}{\linewidth}{XXX}
        \toprule
        \textbf{Section} & \textbf{Method/Model Used} & \textbf{Considerations} \\
        \midrule
        Human Understandable Text & Gemma2:9b & Other LLMs can be used, but using instruction-tuned models is recommended. This is skipped for \textit{FactBench} dataset as discussed on~\ref{subsec:human-understandable-text-generation}. \\
        \hline
        Question Generation & Gemma2:9b & Other LLMs can be used, but using instruction-tuned models is recommended. \\
        \hline
        Question Relevance & Jina-reranker-v1-turbo-en & Cross-encoder models are recommended for this task. \\
        \hline
        Question Relevance Threshold & 0.5 & - \\
        \hline
        Num of Selected Questions & 3 & - \\
        \hline
        Google Search & - & Used query params: \textit{lr} = 'lang\_en', \textit{gl} = 'us', \textit{hl} = 'en', \textit{num} = '100'. The lr parameter is set to the language of the query, gl to the country, hl to the language, and num to the number of results. \\
        \hline
        Num of Selected Documents & 10 & - \\
        \hline
        Document Selection & ms-marco-MiniLM-L-6-v2 & Filtered out the documents from these origins: "dbpedia.org", "wikipedia.org", "wikimedia.org", "wikidata.org", "quora.com", "britannica.com", "scholarpedia.org", "newworldencyclopedia.org", "everipedia.org", "encyclopedia.com", "wikibooks.org", "wiktionary.org", "wikiversity.org", "wikisource.org", "wikiquote.org", "wikivoyage.org", "academia.edu", "nytimes.com" \\
        \hline
        Embedding Model & BAAI/bge-small-en-v1.5 & - \\
        \hline
        Chunking Strategy & Sliding Window with window size 3 & - \\
        \hline
        Similarity Cut-off & Simple & Use the threshold to filter out irrelevant documents. \\
        \hline
        Similarity Cut-off Threshold & 0.3 & - \\
        \hline
        Top\_k & 6 & - \\
        \hline
        Tie-Breaking & - & Based on the models we selected, we use model with higher-param for each model, for Gemma2:9b~$\rightarrow$~27b, Qwen2.5:7b~$\rightarrow$~14b, Llama3.1:8b~$\rightarrow$~70b, and Mistral:7b~$\rightarrow$~Mistral nemo:12b. \\
        \bottomrule
        \caption{System Configurations for Empirical Evaluation} \\
        \label{tab:system-configurations}
    \end{tabularx}
}

%\begin{table}[h!]
%    \footnotesize
%    \begin{xltabular}{\linewidth}{p{3cm}p{2cm}X}
%        \toprule
%        \textbf{Section} & \textbf{\shortstack{Method/\\Model\\Used}} & \textbf{Considerations} \\
%        \midrule
%        Human Understandable Text & Gemma2:9b & Other LLMs can be used, but using instruction-tuned models is recommended. This is skipped for \textit{FactBench} dataset as discussed on~\ref{subsec:human-understandable-text-generation}. \\\hline
%        Question Generation & Gemma2:9b & Other LLMs can be used, but using instruction-tuned models is recommended. \\\hline
%        Question Relevance & Jina-reranker-v1-turbo-en & Cross-encoder models are recommended for this task. \\\hline
%        Question Relevance Threshold & 0.5 & - \\\hline
%        Num of Selected Questions & 3 & - \\\hline
%        Google Search & - & Used query params: \textit{lr} = 'lang\_en', \textit{gl} = 'us', \textit{hl} = 'en', \textit{num} = '100'. The lr parameter is set to the language of the query, gl to the country, hl to the language, and num to the number of results. \\\hline
%        Fact Extraction & - & - \\\hline
%        Num of Selected Documents & 10 & - \\\hline
%        Document Selection & ms-marco-MiniLM-L-6-v2 & Filtered out the documents from these origins: "dbpedia.org", "wikipedia.org", "wikimedia.org", "wikidata.org", "quora.com", "britannica.com", "scholarpedia.org", "newworldencyclopedia.org", "everipedia.org", "encyclopedia.com", "wikibooks.org", "wiktionary.org", "wikiversity.org", "wikisource.org", "wikiquote.org", "wikivoyage.org", "academia.edu", "nytimes.com" \\\hline
%        Embedding Model & BAAI/bge-small-en-v1.5 & - \\\hline
%        Chunking Strategy & Sliding Window with window size 3 & - \\\hline
%        Similarity Cut-off & Simple & Use the threshold to filter out irrelevant documents. \\\hline
%        Similarity Cut-off Threshold & 0.3 & - \\\hline
%        Top\_k & 6 & - \\\hline
%        Tie-Breaking & - & Based on the models we selected, we use model with higher-param for each model, for Gemma2:9b~\rightarrow~27b, Qwen2.5:7b~\rightarrow~14b, Llama3.1:8b~\rightarrow~70b, and Mistral:7b~\rightarrow~Mistral nemo:12b. \\
%        \bottomrule
%    \end{xltabular}
%    \caption{System Configurations for Empirical Evaluation}
%    \label{tab:system-configurations}
%\end{table}

The tests are run on a server with the following specifications:
\begin{itemize}
    \item \textbf{Model Name:} Mac Studio
    \item \textbf{Model Identifier:} Mac14,14
    \item \textbf{Model Number:} Z180000M3T/A
    \item \textbf{Chip:} Apple M2 Ultra
    \item \textbf{Total Number of Cores:} 24 (16 performance and 8 efficiency)
    \item \textbf{Memory:} 192 GB
    \item \textbf{System Firmware Version:} 11881.1.1
    \item \textbf{OS Loader Version:} 11881.1.1
\end{itemize}
The timing tests are conducted over 50 samples for each section, using a MacBook Pro equipped with an Apple M2 Max chip and 32 GB of memory.

\section{Comparative Analysis}\label{sec:empirical-evaluation:comparative-analysis}
\begin{table}[ht!]
    \noindent
    {\scriptsize ms-marco-MiniLM-L-6-v2, BAAI/bge-small-en-v1.5, Sliding Window (ws 3), Similarity Cut-off (Original), Top\_k 6}
    \resizebox{\textwidth}{!}{
        \begin{threeparttable}
        \begin{tabular}{llccc||cc}
            \toprule
            \textbf{Dataset}            & \textbf{Model}                     & \textbf{Stability}             & \textbf{Avg. Request Duration}\tnote{*} & \textbf{Avg. tokens per request} & \textbf{ACC} & \textbf{F1} \\
            \midrule
            \multirow{6}{*}{FactBench}  & Gemma2                             & 0.8720                         & 5.6826s                            & 1605.29                                       & \textbf{0.9032}    & \textbf{0.9101}   \\
                                        & Qwen2.5                            & 0.8685                         & 6.3094s                            & 1652.66                                       & 0.8729    & 0.8888   \\
                                        & LLama3.1                           & 0.8291                         & 6.5270s                            & 1679.04                                       & 0.8154    & 0.8299   \\
                                        & Mistral                            & 0.8650                         & 4.6692s                            & 1594.76                                       & 0.8511    & 0.8733   \\ \cline{2-7}
                                        & Proposed (At\_Most)                & \textbf{0.9176}                & 16.815s                            & 1604.196                                       & 0.8964    & 0.9071   \\
                                        & Proposed (At\_Least)               & N/A                            & N/A                            & N/A                                       & N/A    & N/A   \\ \hline \hline
            \multirow{6}{*}{YAGO}       & Gemma2                             & 0.8785                         & 5.1982s                            & 1597.50                                       & 0.8499    & 0.9187   \\
                                        & Qwen2.5                            & 0.8773                         & 6.2770s                            & 1653.56                                       & 0.8506    & 0.9191   \\
                                        & LLama3.1                           & 0.8276                         & 6.4070s                            & 1682.78                                       & 0.8059    & 0.8922   \\
                                        & Mistral                            & 0.8817                         & 4.4824s                            & 1589.58                                       & \textbf{0.9250}    & \textbf{0.9610}   \\ \cline{2-7}
                                        & Proposed (At\_Most)                & \textbf{0.9226}                & 6.4810s                            & 1586.02                                       & 0.8737    & 0.9325   \\
                                        & Proposed (At\_Least)               & N/A                            & N/A                            & N/A                                       & N/A    & N/A   \\ \hline \hline
            \multirow{6}{*}{DBpedia}    & Gemma2                             & N/A                          & N/A                            & N/A                                       & \textbf{N/A}    & \textbf{N/A }   \\
                                        & Qwen2.5                            & N/A                          & N/A                            & N/A                                       & N/A     & N/A    \\
                                        & LLama3.1                           & N/A                          & N/A                            & N/A                                       & N/A     & N/A    \\
                                        & Mistral                            & N/A                          & N/A                            & N/A                                       & N/A     & N/A    \\ \cline{2-7}
                                        & Proposed (At\_Most)                & \textbf{N/A }                & N/A                            & N/A                                       & N/A     & N/A    \\
                                        & Proposed (At\_Least)               & \textbf{N/A }                & N/A                            & N/A                                       & N/A     & N/A    \\

            \bottomrule
        \end{tabular}
        \begin{tablenotes}
            \item[*] Each query uses two requests, so the average request duration and average tokens per request are calculated based on the two requests.
        \end{tablenotes}
        \end{threeparttable}}
    \caption{Empirical Evaluation Results of the Proposed System and Candidate Models over the Datasets}
    \label{tab:evaluation_results-full-wo-category-all-datasets}
\end{table}


\subsection{Discussion of Results}\label{subsec:discussion-of-results}
The empirical evaluation results presented in Table~\ref{tab:evaluation_results-full-wo-category-all-datasets} offer valuable insights into the performance of our proposed system and the individual candidate models across the three datasets: \textit{FactBench}, \textit{YAGO}, and \textit{DBpedia}.

Among the candidate models, \textit{Llama3.1} generates the reasoning most of the time and don't follow the instructions fully, this way the Avg completion tokens is higher than the other models.

In terms of overall accuracy and F1 scores, the \textit{Gemma2} model outperforms the other individual models on the FactBench dataset.
This superior performance can be attributed to Gemma2's advanced attention mechanisms, which employ a hybrid approach of sliding window attention and global attention to effectively capture both local and long-range dependencies in the input sequences.
The model's ability to learn from larger teacher models during pre-training and its use of on-policy distillation post-training likely contribute to its strong performance relative to its size.

Interestingly, the \textit{Gemma2} model achieves the highest accuracy and F1 scores on the YAGO dataset, surpassing even \textit{Gemma2}.
This may be due to Mistral's optimized transformer architecture, which incorporates innovations such as Grouped-Query Attention and Sliding Window Attention to improve inference speed and handle longer sequences more effectively.

The \textit{Qwen2.5} and \textit{LLama3.1} models generally fall in the middle of the pack in terms of accuracy and F1 scores.
While both models leverage large-scale pre-training and instruction tuning, their performance suggests room for improvement in the specific task of knowledge graph fact verification.

Notably, our proposed system, which employs an ensemble approach using the combination strategy, consistently achieves competitive accuracy and F1 scores compared to the best-performing individual models.
This highlights the effectiveness of combining the strengths of multiple models through majority voting and adaptive dispute resolution techniques.
However, the increased computational cost of running multiple models in parallel is reflected in the higher average request duration for the proposed system.
The stability scores provide another dimension for comparison, indicating the consistency of model outputs across different runs.
The proposed system achieves the highest stability scores on datasets, showing the robustness gained by synthesizing results from multiple models.

%TODO: CHECK WHEN DBPEDIA COMPLETED
Among the individual models, \textit{Gemma2} and \textit{Mistral} exhibit relatively high stability, while \textit{LLama3.1} has the lowest stability scores.
\subsection{Key Findings}\label{subsec:empirical-evaluation:discussion-of-results:key-findings}
\subsection{Error Analysis}\label{subsec:empirical-evaluation:discussion-of-results:error-analysis}
